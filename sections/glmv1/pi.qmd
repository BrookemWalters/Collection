---
title: ""
format: 
  html:
    code-fold: true
---

![](/images/logos/logo_white.png)

```{r include=FALSE}
setwd("~/gvsu/summer 23/stat 631/Collection/sections/glmv1")

if (!require("pacman")) {
  install.packages("pacman")
  library("pacman")
}
#Import Libraries

# Use p_load() to install (if necessary) and load  packages
pacman::p_load(tidyverse, # for data cleaning and preparing
               tidymodels, # for modeling
               leaps, # for creating lags
               MMWRweek,# date time groups
               broom, # tidy() feature
               gt, # making tables
               GGally, # for comparing variables
               poissonreg) # parsnips for poisson

```

DRAFT DRAFT LESS OUTLINE MORE WRITING

# Project Overview

Throughout this project, I showcase the practical application of predictive analytics by utilizing statistical methods. I focus on unraveling temporal sales patterns to anticipate future consumer demand. Ultimately, this project demonstrates how analytics fuels the decision-making processes with forecasts and strategic insights.

While the primary aim of the project is demonstrating the foundations of statistical modeling, it also serves as a platform for expanding other essential data science competencies. Leveraging R, I've not only engineered novel data sets, but also enhanced my knowledge in the areas of data wrangling and visualization design.

Overall, this project showcases more than current competencies; rather, it represents my potential for continuous growth and adaptability in the ever-evolving field of data science and machine learning. It marks the acquisition of new skills developed over the course of the semester, complementing my years spent cultivating my craft of compelling data storytelling.

[insert image final ]
**goal**: forecast the inventory for the upcoming year

**limitations**: ideally, time series predictions benefit from multi-year trending data sets. However, the data for this project encompasses merely 54 weeks.

Additionally, my understanding of the data is constrained. For this project, I made certain assumptions about the time frame and inventory type. In practical scenarios, thorough consultation and collaboration with domain experts are essential components of any predictive analytics project.

![](/images/AdobeStock_184864827.jpeg) **this report contains:**

-   **Data Processing**:
    -   feature engineering
    -   data splitting into training and testing sets  
-   **Model Development**:
    -   step-wise model selection methodology  
-   **Analytics and Evaluation**:
    -   exploratory and descriptive analytics
    -   interpretation and evaluation of model outcomes

\

## Initial Insights:

The inventory data is comprised of 488 distinct items, sold over a span of 54 weeks (from week 0 to 53), with an impressive sum of approximately 1.3 million units transacted.

To visualize and better understand the distribution of sales, I prepared a series of graphical representations to discern patterns, identify anomalies, and appreciate the overall behavior of our sales data over time, thereby setting the stage for the predictive analysis.

```{r fig.height=4, fig.width=10, message=FALSE, warning=FALSE}

# import data
sai <-  read.csv("inventory.csv")
# import my custom theme :-)
source("artNov_theme.R")

hist <- sai %>% 
  group_by(item_no) %>%
  summarise(total_sold = sum(sold)) %>% 
 
ggplot(aes(x = total_sold)) +
  geom_histogram(binwidth = 2100, color = "black", fill = "#7D998F", 
                 alpha = .6) +
        scale_x_continuous(labels = scales::comma_format()) +
  
  labs(x = "Number of Items Sold", y = "Frequency", 
       title = "Number of Items Sold by Item Type", 
       caption =  " figure 1.1")+
    theme_art_nouveau()



hist1 <- sai %>% 
  group_by(week) %>%
  summarise(total_sold = sum(sold)) %>% 
 
ggplot(aes(x = total_sold)) +
  geom_histogram(binwidth =  2100, color = "black", fill = "#68576D", 
                 alpha = .6) +
        scale_x_continuous(labels = scales::comma_format()) +
        scale_y_continuous(position = "right") +
  
  labs(x = "Number of Items Sold", 
       title = "Number of Items Sold by Week",
        caption =  " figure 1.2")+
  theme_art_nouveau()

# display the histograms side by side
gridExtra::grid.arrange(hist, hist1, ncol = 2)
```

###### **figure 1.** *The histograms demonstrate the distribution of the dataset by variable. The green histogram demonstrates the frequency of each item sold, with most items ranging between 500 and 1300 in total. The pink histogram conveys the temporal dispersion of weekly sales. Both graphs exhibit a right skew, particularly pronounced in the green histogram, implying a scarcity of high-sales items or weeks.*

\
\
\
<br>

```{r fig.height=4, fig.width=10, warning=FALSE}
line1 <- sai %>%
  group_by(week) %>%
  summarise(total_sold = sum(sold)) %>%
  ggplot(aes(x = week, y = total_sold)) +
  geom_line(size = 2, alpha = 0.5, color = "#DF5875") +
  geom_point(size = 4, alpha = 0.5, color = "#DF5875")+
  geom_area(fill = "#DF5875", alpha = 0.4) +
  scale_x_continuous(breaks = seq(0, 55, by = 5)) + 
  scale_y_continuous(labels = scales::comma_format()) +
   
  labs(x = "week", y = "Count of Items Sold", 
       title = "Number of Items Sold by Week",
        caption =  " figure 2")+
  theme_art_nouveau()
line1
```

###### **figure 2.** *The line graph shows annual sales trends in the unprocessed data set. Weekly sales fluctuate within a range of 10,000 to 50,000 items, with the majority hovering between \~20k and \~30k units sold. A distinct sales peak is observed in week 20, while week 46 registers the slowest sales activity. This trend analysis aids in identifying key sales periods and provides valuable context for refining our predictive models*

\
\
\
<br>

## Data Processing

I feature engineered new variables to derive a better understanding of sales trends and enhance the predictive model's performance. These variables provide valuable insights into sales trends, seasonality, and best-selling items, which can be used to enhance the predictive model's performance and gain a better understanding of the data's dynamics. Ideally, meeting with domain experts would be part of this process, to ensure the variables represent reality.

### data dictionary

-   **item_no**: the unique identifier for each item
-   **week**: the week number. "sold": number of items sold during a specific week
-   **year_total**:the total number of items sold for each item over the entire year
-   **date**: The specific date corresponding to a week
-   **month**: the month corresponding to a specific week
-   **lag1**: period for capturing seasonality or time-dependent patterns, item's sold from the previous week
-   **lag2**: item's sold from two-weeks prior
-   **rolling_4wk_avg**: the total number of items sold over the past four weeksby item_no
-   **best_seller**: A binary variable indicating whether the item is one of the top ten best-selling items based on volume

```{r fig.height=4, fig.width=10}

#####
# create the variables
# total yearly sales for each item
inventory_sum <- sai %>%
  group_by(item_no) %>%
  summarise(year_total = sum(sold), .groups = "drop")

sai <- left_join(sai, inventory_sum, by = c("item_no"))

###
# calculate the top 10% threshold
top_10_threshold <- quantile(sai$year_total, 0.9)

# dummy variable for bestsellers 1= yes 0 = no
sai <- sai %>%
  mutate(best_seller = ifelse(year_total >= top_10_threshold, 1, 0))

## figure out month an year for data, assuming the data is from  2022
# week 0: December 27, 2021 - January 2, 2022
# week 52: December 19, 2022 - December 25, 2022
# week 53: December 26, 2022 - January 1, 2023

sai$year <- 2022

sai <- sai %>%
  mutate(calendar_week = ifelse(week == 0, 52, ifelse(week == 53, 1, week)),
         calendar_year = ifelse(week == 0, year - 1, ifelse(week == 53, year + 1, year)))

# Now use calendar_year and calendar_week in the MMWRweek2Date function
sai$date <- MMWRweek2Date(sai$calendar_year, sai$calendar_week)



# find the year the week ends in, so we can figure out months
sai <- sai %>%
  mutate(wk_ending_year = ifelse(week %in% c(0, 53), year + 1, year),
         week = ifelse(week == 53, 1, week))



# create teh data and extract the month
# wanted month to be quantitative to reduce model complexity

sai$date <- MMWRweek2Date(sai$wk_ending_year, sai$calendar_week)
sai$month <- month(sai$date, label = TRUE)
sai$month <- month(sai$date)


# create lag periods
# https://www.youtube.com/watch?v=Kn3llTjYS5E

sai <- sai %>%
  mutate(lag1 = lag(sold, 1),
         lag2 = lag(sold, 2))

# https://www.rdocumentation.org/packages/zoo/versions/1.8-12/topics/rollmean
sai <- sai %>%
  group_by(item_no) %>%
  mutate(rolling_4wk_avg = zoo::rollmean(sold, k = 4, fill = NA, align = "right")) %>%
  ungroup()

write.csv(sai, "FEinventory.csv")




month_abbreviations <- c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec")

sai_average <-  sai %>%
  filter(wk_ending_year == 2022) %>% 
  group_by(month) %>% 
  summarise(month_average = mean(sold))



bar <- sai %>%
    filter(wk_ending_year == 2022) %>% 
    mutate(month = factor(month, levels = 1:12, labels = month_abbreviations)) %>%
  ggplot(aes(x = month, y = sold)) +
  geom_bar(stat = "sum", fill = "#68576D", alpha = 0.75) +
  labs(x = "Month", y = "Total Sold", title = "Total Sold by Month",
       caption =  " figure 3.1") +
  scale_y_continuous(labels = scales::comma_format()) +
  theme_art_nouveau() +
  theme(legend.position = "none")

dot <-  sai_average %>%
  mutate(month = factor(month, levels = 1:12, labels = month_abbreviations)) %>%
  ggplot(aes(x = month, y = month_average)) +
  geom_point(size = 3) +
  geom_point(size = 4, shape = 8, alpha = 0.75) +
  geom_text(aes(label = round(month_average, 2)), nudge_y = 1.5) +  #closer annotations
  scale_y_continuous(position = "right") +
  labs(x = "Month", y = "", title = "Average Items Sold by Month", caption = "figure 3.2") +
  theme_art_nouveau() +
  theme(axis.text.y = element_blank())  
 
  

gridExtra::grid.arrange(bar, dot, ncol = 2)

```

###### **figure 3.** *The bar graph displays the total sales volume by month. July stands out as the peak season, whereas November and December show lower inventory movement. U. However, the average number of items sold in a month is relatively low compared to the totals. This suggests the presence of numerous zeros, which are affecting the overall average.*

```{r warning=FALSE}
# #https://www.geeksforgeeks.org/visualization-of-a-correlation-matrix-using-ggplot2-in-r/
# theme_art_nov_minimal <- function() {
#   theme(
#     text = element_text(family = 'EB Garamond'),
#     panel.grid = element_blank(),
#     panel.border = element_blank(),
#     panel.background = element_rect(fill = "#FEE8E1", color = "#DF5875"),
#     panel.grid.major = element_line(color = "#DF5875", linetype = "dotted", linewidth = 0.5),
#     panel.grid.minor = element_blank(),
#     axis.line = element_line(color = "#DF5875", linewidth = 1),
#     legend.position = "bottom"
#   )
# }
# 
# 
# sai %>%
#   select(week, month, lag1, lag2, rolling_4wk_avg, best_seller, sold) %>%
#   ggpairs(
#     upper = list(continuous = "cor"),
#     lower = list(continuous = "points"),
#     diag = list(continuous = "densityDiag")
#   ) +
#   theme_art_nov_minimal()
```

###### **figure 4.** *The corelation matrix.*

\
\
\
<br> 


### Splitting the Data 

With the variables now set, it's time to divide the data for the training and testing of the predictive model. Ideally, the previous year would serve as a training ground, providing a foundation for testing the model. However, due to the critical nature of maintaining the temporal order, I segmented the data by inventory type, 75% into training to test on the remaining 25%.

```{r warning=FALSE}
# create a list of all items to randomly select
set.seed(1985)
items_sample_list <-  unique(sai$item_no) %>% 
  sample(366)
# 75% of the data


# splitting the inventory by item type to keep the temporal order

is_train <-  sai %>% 
  filter(item_no %in% items_sample_list) %>% 
  dplyr::select(
    item_no,
    week,
    month,
    lag1,
    lag2,
    rolling_4wk_avg,
    best_seller, 
    sold) #not sure why i had to include the dplyr package


is_test<-  sai %>% 
  filter(!(item_no %in% items_sample_list)) %>% 
  dplyr::select(
    item_no,
    week,
    month,
    lag1,
    lag2,
    rolling_4wk_avg,
    best_seller, 
    sold) #not sure why i had to include the dplyr package


## story telling purposes only
# https://r-graph-gallery.com/piechart-ggplot2.html
piecha <- data.frame(
  split = c("train", "test"),
  percentage = c(0.75, 0.25)
)

piecha <- piecha %>%
  arrange(desc(split)) %>%
  mutate(cumulative_percentage = cumsum(percentage) - percentage / 2,
         label = paste(split, ": ", scales::percent(percentage)))

# do you like fancy pie?
ggplot(piecha , aes(x="", y=percentage, fill=split, label = label)) +
  geom_bar(stat="identity", width=2, color="white", alpha=0.6) +
  coord_polar("y", start=0) +
  scale_fill_manual(values=c("#68576D", "#DF5875"), guide = "none") +
  theme_art_nouveau() +
  geom_text(aes(y = cumulative_percentage), color = "white", size = 7, face = "bold") +
  theme(axis.title = element_blank(),
        axis.text = element_blank()) +
  labs(title = "The Data Split",
              caption = "figure 5")
  theme(plot.title = element_text(hjust = 0.5))
```
\
\
\
<br>



## Model Feature Selection
The next step of the project involves selecting the best model using stepwise selection. It's an iterative process of adding and removing predictors based on their statistical significance. However, it should be used with caution. The inclusion of irrelevant variables can lead to unnecessary complexity in the resulting model and can inflate the variance of prediction errors.

```{r message=FALSE, warning=FALSE}

# https://youtu.be/IScjygOnO0w
# forward Stepwise Selection
# nvmax = 8 specifies the maximum number of predictors to incorporate in the model.
is_mod <- regsubsets(sold ~ week + month + lag1 + lag2 + rolling_4wk_avg + best_seller,
                       data = is_test, nbest = 2, method = "exhaustive")
is_mod_summary_mx <- with(summary(is_mod), data.frame(rsq,adjr2, cp, rss, outmat))

is_mod_summary_mx$predictors <- c(1,1,2,2,3,3,4,4,5,5,6) # for graphing
is_mod_summary_mx$model <- c('a','b','a','b','a','b','a','b','a', 'b', 'a') # for graphing

# update this too  look more like my other graphs
#https://posit.co/blog/great-looking-tables-gt-0-2/# turn results into a pretty table
# is_mod_summary_mx

is_mod_summary_mx %>% 
  mutate(rsq = round(rsq, 1), 
         adjr2 = round(adjr2, 1), 
         cp = scales::comma(round(cp, 1)),
         rss = scales::comma(rss)) %>%
  rename(`4WK ROLLING AVG` = rolling_4wk_avg, `BEST SELLER` = best_seller) %>% 
  select(predictors, model, everything()) %>%
  gt(rowname_col = "predictors") %>%
  tab_header(title = md("Model Comparison")) %>% 
    tab_options(
    grand_summary_row.background.color = "#D35C9E",
    heading.background.color = "#EFFBFC",
    column_labels.background.color = "#EFFBFC",
    stub.background.color = "#EFFBFC",
    table.font.color = "#323232",
    stub.border.style = "dashed",
    stub.border.width = "1px",
    table.width = "60%") %>%
    tab_spanner(
    label = "predictors ",
    columns = c(week, month, lag1, lag2, `4WK ROLLING AVG`, `BEST SELLER`)) %>%
 
  cols_align(align = "center", columns = cp) %>% 
  cols_align(align = "center", columns = rss) %>%
  opt_all_caps() %>% 
  opt_table_font(
    font = google_font(name = "EB Garamond"))
 # caption = table 1 
```

###### **table 1. the stepwise selection went through an exhaustive process to chose the best 2 models by number of predictors, explain xyx**

### Model Evaluation

```{r fig.height=4, fig.width=10}

# make tghis chart prettier
gcolors <- c("#68576D", "#DF5875")

is_mod_summary_mx %>%
  pivot_longer(c(adjr2, cp), names_to = "metric", values_to = "value") %>%
  ggplot(aes(x = predictors, y = value, color = model)) +
  geom_line(show.legend = FALSE, size = 1, alpha = .5 ) +
    geom_point(size = 4, alpha = 0.5)+
  facet_wrap(~metric, scales = "free") +
  scale_x_continuous(breaks = 1:6) +
  scale_color_manual(values = gcolors) +
  labs(x = "predictors", y = "metric", title = "Number of Predictors by ADJR2 and CP",
       caption =  " figure 6") +
  theme_art_nouveau() 




# how can i make the value label larger on this?
```
###### **Figure 6. explain this graph **


I see that the more predictors the great the adjr2 and the lower the cp, which is be expected. there appears to be diminishing returns at 4 predictors.

model a has more optimal score on this compared to model b

why too many predictors are bad reason xyz

\[interpret the graph\]

explain why this is the best one what do each of these things tell me? - rsq -\> highest r squared, explains the variability in the model - adjr2 - \> highest - cp -\> smallest - rss -\> smallest

I want to take my models through one more level os scrunity. having trouble choosing, i spent a lot of time making my variables so lets look at AIC several models what is AIC? Lower AIC values indicate a better-fit model, and a model with a delta-AIC (the difference between the two AIC values being compared) of more than -2 is considered significantly better than the model it is being compared to

find the coefficients and (Akaike Information Criterion) AIC Multiple linear regression compare the three best models

```{r}
#https://emilhvitfeldt.github.io/ISLR-tidymodels-labs/03-linear-regression.html

# tidy model way

 specs <- linear_reg() %>%
  set_mode("regression") %>%
  set_engine("lm")

lm_3 <- specs %>%
  fit(sold~ lag1 + lag2 + rolling_4wk_avg , data = is_train)

lm_4 <- specs %>%
  fit(sold~ month + lag1 + lag2 + rolling_4wk_avg , data = is_train)

lm_5 <- specs %>%
  fit(sold~ month + week + lag1 + lag2 + rolling_4wk_avg , data = is_train)

mod_compare <-  rbind(glance(lm_3),glance(lm_4),glance(lm_5))
```

```{r}

#
mod_compare %>% 
 gt(rowname_col = "df") %>%
  tab_header(title = md("model comparison")) %>% 
  cols_align(align = "left") %>% 
    tab_options(
    grand_summary_row.background.color = "#D35C9E",
    heading.background.color = "#EFFBFC",
    column_labels.background.color = "#EFFBFC",
    stub.background.color = "#EFFBFC",
    table.font.color = "#323232",
    stub.border.style = "dashed",
    stub.border.width = "1px",
    table.width = "60%"
  ) %>%
  opt_all_caps() 

 # caption = table 2

# 
# is_mod_summary_mx %>% 
#   mutate(rsq = round(rsq, 1), 
#          adjr2 = round(adjr2, 1), 
#          cp = scales::comma(round(cp, 1)),
#          rss = scales::comma(rss)) %>%
#   rename(`4WK ROLLING AVG` = rolling_4wk_avg, `BEST SELLER` = best_seller) %>% 
#   select(predictors, model, everything()) %>%
#   gt(rowname_col = "predictors") %>%
#   tab_header(title = md("Model Comparison")) %>% 
#     tab_options(
#     grand_summary_row.background.color = "#D35C9E",
#     heading.background.color = "#EFFBFC",
#     column_labels.background.color = "#EFFBFC",
#     stub.background.color = "#EFFBFC",
#     table.font.color = "#323232",
#     stub.border.style = "dashed",
#     stub.border.width = "1px",
#     table.width = "60%") %>%
#     tab_spanner(
#     label = "predictors ",
#     columns = c(week, month, lag1, lag2, `4WK ROLLING AVG`, `BEST SELLER`)) %>%
#  
#   cols_align(align = "center", columns = cp) %>% 
#   cols_align(align = "center", columns = rss) %>%
#   opt_all_caps() %>% 
#   opt_table_font(
#     font = google_font(name = "EB Garamond"))
# 






```
###### **table 2. explain this table **

interpret the results

```{r}
train_precict <- bind_cols(
  predict(lm_3, new_data = is_train, type = "conf_int"),
  is_train
) %>%
  select(item_no, week, sold, .pred_lower, .pred_upper)

# graph the results
```
###### **Figure 7. explain this graph **
compare the train model against the test

```{r}
lm_3 <- specs %>%
  fit(sold~ lag1 + lag2 + rolling_4wk_avg , data = is_train)

lm_3t <- specs %>%
  fit(sold~ lag1 + lag2 + rolling_4wk_avg , data = is_test)

tt_compare <-  rbind(glance(lm_3),glance(lm_3t))
tt_compare$mod <-  c("train", "test")

# create a visualization
```

```{r}
actual_predicted <- bind_cols(
  predict(lm_3t, new_data = is_train, type = "conf_int"),
  is_train
) %>%
  select(item_no, week, sold, .pred_lower, .pred_upper)
```

###### **Figure 8. explain this graph **

### Conclusion



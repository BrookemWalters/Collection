---
title: ""
format: 
  html:
    code-fold: true
---

![](/images/logos/logo_white.png)

```{r include=FALSE}
setwd("~/gvsu/summer 23/stat 631/Collection/sections/glmv1")

if (!require("pacman")) {
  install.packages("pacman")
  library("pacman")
}
#Import Libraries

# Use p_load() to install (if necessary) and load  packages
pacman::p_load(tidyverse, # for data cleaning and preparing
               tidymodels, # for modeling
               leaps, # for creating lags
               MMWRweek,# date time groups
               broom, # tidy() feature
               gt, # making tables
               GGally, # for comparing variables
               poissonreg) # parsnips for poisson

```

DRAFT DRAFT LESS OUTLINE MORE WRITING

# Project Overview

"Welcome to my Stats 631[^1] final project!

[^1]:
    ## **STA 631 - STATISTICAL MODELING AND REGRESSION**

    Traditional and modern computationally intensive statistical modeling techniques. Basics of probability theory, including conditional probability, Bayes' Theorem, and univariate probability models. Regression modeling and prediction including simple linear, multiple, logistic, Poisson, and nonlinear and nonparametric regression. Methods for model selection and shrinkage. Emphasis is on application and interpretation using statistical software. Offered fall semester. Prerequisite: Admission into the data science professional science master's program. https://www.gvsu.edu/catalog/course/sta-631.htm

This digital portfolio stands as a testament to my proficiency in applied statistical methods and showcases my capacity for ongoing growth and adaptability in the ever-evolving domains of data science and machine learning. During the summer semester of 2023, I not only broadened my statistical knowledge but also cultivated my craft of data storytelling.

At its heart, this project is one hypothetical case study for predictive statistical modeling. Leveraging the R programming language, I conducted model selection and broadened my expertise in data wrangling, visualization, and design aesthetics.

More specifically, this document guides you through a time series inventory prediction--- the art of anticipating future consumer demand. Ultimately, it demonstrates how data empowers the decision-making process through forecasting and strategic insights. And remember,

![](/images/quotes_george-box_models-wrong-some-useful_tribal-simplicity.png){width="500"}\
\
\
<br>

## Summary

**goal**: forecast the inventory for the upcoming year

**limitations**: the data set contains only spans 54 weeks, which is less than ideal for time series predictions that typically benefit from multi-year trends.

Additionally, my understanding of the data is limited. For this project, I made certain assumptions about the time frame and inventory type. In practical scenarios, collaboration with domain experts are essential components of any predictive analytics project.

\
\
<br>

**this report contains:**

-   [**Data Processing**](https://brookemwalters.github.io/Collection/sections/glmv1/pi.html#data-processing)
    -   feature engineering
    -   data splitting into training and testing sets  
-   [**Model Development**](https://brookemwalters.github.io/Collection/sections/glmv1/pi.html#model-development)
    -   step-wise model selection methodology  
-   [**Analytics and Evaluation**](https://brookemwalters.github.io/Collection/sections/glmv1/pi.html#analytical-and-evaluation)
    -   exploratory and descriptive analytics
    -   interpretation and evaluation of model outcomes

![](/images/AdobeStock_184864827.jpeg)

## Initial Insights:

The inventory data is comprised of 488 distinct items, sold over a span of 54 weeks (from week 0 to 53), with an impressive sum of approximately 1.3 million units transacted.

To visualize and better understand the distribution of sales, I prepared a series of graphical representations to discern patterns, identify anomalies, and appreciate the overall behavior of the inventory data over time, thereby setting the stage for the predictive analysis.

```{r fig.height=4, fig.width=10, message=FALSE, warning=FALSE}

# import data
sai <-  read.csv("inventory.csv")
# import my custom theme :-)
source("artNov_theme.R")

hist <- sai %>% 
  group_by(item_no) %>%
  summarise(total_sold = sum(sold)) %>% 
 
ggplot(aes(x = total_sold)) +
  geom_histogram(binwidth = 2100, color = "black", fill = "#7D998F", 
                 alpha = .6) +
        scale_x_continuous(labels = scales::comma_format()) +
  
  labs(x = "Number of Items Sold", y = "Frequency", 
       title = "Number of Items Sold by Item Type", 
       caption =  " figure 1.1")+
    theme_art_nouveau()



hist1 <- sai %>% 
  group_by(week) %>%
  summarise(total_sold = sum(sold)) %>% 
 
ggplot(aes(x = total_sold)) +
  geom_histogram(binwidth =  2100, color = "black", fill = "#68576D", 
                 alpha = .6) +
        scale_x_continuous(labels = scales::comma_format()) +
        scale_y_continuous(position = "right") +
  
  labs(x = "Number of Items Sold", 
       title = "Number of Items Sold by Week",
        caption =  " figure 1.2")+
  theme_art_nouveau()

# display the histograms side by side
gridExtra::grid.arrange(hist, hist1, ncol = 2)
```

###### **figure 1.** *The histograms demonstrate the distribution of the dataset by variable. Figure 1.1 demonstrates the frequency of each item sold, with most items ranging between 500 and 1,300 in total. The figure 1.2 conveys the temporal dispersion of weekly sales. Both graphs exhibit a right skew, particularly pronounced in the figure 1.1, implying a scarcity of high-sales items or weeks.*

\
\
\
<br>

```{r fig.height=4, fig.width=10, warning=FALSE}
line1 <- sai %>%
  group_by(week) %>%
  summarise(total_sold = sum(sold)) %>%
  ggplot(aes(x = week, y = total_sold)) +
  geom_line(size = 2, alpha = 0.5, color = "#DF5875") +
  geom_point(size = 4, alpha = 0.5, color = "#DF5875")+
  geom_area(fill = "#DF5875", alpha = 0.4) +
  scale_x_continuous(breaks = seq(0, 55, by = 5)) + 
  scale_y_continuous(labels = scales::comma_format()) +
   
  labs(x = "week", y = "Count of Items Sold", 
       title = "Number of Items Sold by Week",
        caption =  " figure 2")+
  theme_art_nouveau()
line1
```

###### **figure 2.** *shows annual sales trends on a weekly cadence. Weekly sales fluctuate within a range of 10,000 to 50,000 items, with the majority hovering between \~20k and \~30k units sold. A sales peak is observed in week 20, while week 46 registers the slowest sales activity. This trend analysis aids in identifying key sales periods and provides context for the predictive analysis.*

\
\
\
<br>

## Data Processing

For improved sales trend insights and predictive model performance, I created new features. These variables reveal sales dynamics, exposing potential seasonal pattern influences. Ideally, this process includes consultation with domain experts to ensure variable viability.

### data dictionary

-   **Original**
    -   **item_no**: the unique identifier for each item
    -   **week**: the week number. "sold": number of items sold during a specific week
    -   **sold**: count of items_no sold in a period\
         
-   **Feature Engineered**
    -   **year_total**: the total number of items sold for each item over the entire year
    -   **date**: The specific date corresponding to a week
    -   **month**: the month corresponding to a specific week
    -   **lag1**: period for capturing seasonality or time-dependent patterns, item's sold from the previous week
    -   **lag2**: item's sold from two-weeks prior
    -   **rolling_4wk_avg**: the total number of items sold over the past four weeks by item_no
    -   **best_seller**: A binary variable indicating whether the item is one of the top ten best-selling items based on volume

```{r fig.height=4, fig.width=10}

#####
# create the variables
# total yearly sales for each item
inventory_sum <- sai %>%
  group_by(item_no) %>%
  summarise(year_total = sum(sold), .groups = "drop")

sai <- left_join(sai, inventory_sum, by = c("item_no"))

###
# calculate the top 10% threshold
top_10_threshold <- quantile(sai$year_total, 0.9)

# dummy variable for bestsellers 1= yes 0 = no
sai <- sai %>%
  mutate(best_seller = ifelse(year_total >= top_10_threshold, 1, 0))

## figure out month an year for data, assuming the data is from  2022
# week 0: December 27, 2021 - January 2, 2022
# week 52: December 19, 2022 - December 25, 2022
# week 53: December 26, 2022 - January 1, 2023

sai$year <- 2022

sai <- sai %>%
  mutate(calendar_week = ifelse(week == 0, 52, ifelse(week == 53, 1, week)),
         calendar_year = ifelse(week == 0, year - 1, ifelse(week == 53, year + 1, year)))

# Now use calendar_year and calendar_week in the MMWRweek2Date function
sai$date <- MMWRweek2Date(sai$calendar_year, sai$calendar_week)



# find the year the week ends in, so we can figure out months
sai <- sai %>%
  mutate(wk_ending_year = ifelse(week %in% c(0, 53), year + 1, year),
         week = ifelse(week == 53, 1, week))



# create teh data and extract the month
# wanted month to be quantitative to reduce model complexity

sai$date <- MMWRweek2Date(sai$wk_ending_year, sai$calendar_week)
sai$month <- month(sai$date, label = TRUE)
sai$month <- month(sai$date)


# create lag periods
# https://www.youtube.com/watch?v=Kn3llTjYS5E

sai <- sai %>%
  mutate(lag1 = lag(sold, 1),
         lag2 = lag(sold, 2))

# https://www.rdocumentation.org/packages/zoo/versions/1.8-12/topics/rollmean
sai <- sai %>%
  group_by(item_no) %>%
  mutate(rolling_4wk_avg = zoo::rollmean(sold, k = 4, fill = NA, align = "right")) %>%
  ungroup()

write.csv(sai, "FEinventory.csv")




month_abbreviations <- c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec")

sai_average <-  sai %>%
  filter(wk_ending_year == 2022) %>% 
  group_by(month) %>% 
  summarise(month_average = mean(sold))



bar <- sai %>%
    filter(wk_ending_year == 2022) %>% 
    mutate(month = factor(month, levels = 1:12, labels = month_abbreviations)) %>%
  ggplot(aes(x = month, y = sold)) +
  geom_bar(stat = "sum", fill = "#DF5875", alpha = 0.75) +
  labs(x = "Month", y = "Total Sold", title = "Total Sold by Month",
       caption =  " figure 3.1") +
  scale_y_continuous(labels = scales::comma_format()) +
  theme_art_nouveau() +
  theme(legend.position = "none")

dot <-  sai_average %>%
  mutate(month = factor(month, levels = 1:12, labels = month_abbreviations)) %>%
  ggplot(aes(x = month, y = month_average)) +
  geom_point(size = 3) +
  geom_point(size = 4, shape = 8, alpha = 0.75) +
  geom_text(aes(label = round(month_average, 2)), nudge_y = 1.5) +  #closer annotations
  scale_y_continuous(position = "right") +
  labs(x = "Month", y = "", title = "Average Items Sold by Month", caption = "figure 3.2") +
  theme_art_nouveau() +
  theme(axis.text.y = element_blank())  
 
  

gridExtra::grid.arrange(bar, dot, ncol = 2)

```

###### **figure 3.** *The bar graph displays the total sales volume by month. July stands out as the peak season, whereas November and December show lower inventory movement. However, the average number of items sold in a month is relatively low compared to the totals. This suggests the presence of numerous zeros, which are affecting the overall average.*

\
\
\
<br>

```{r  fig.height=4, fig.width=10, message=FALSE, warning=FALSE}
#https://www.geeksforgeeks.org/visualization-of-a-correlation-matrix-using-ggplot2-in-r/

sai %>%
  select(week, month, lag1, lag2, rolling_4wk_avg, best_seller, sold) %>%
  ggpairs(
    upper = list(continuous = wrap("cor", size = 5)),
    lower = list(continuous = "points"),
    diag = list(continuous = "densityDiag")
  ) +
  theme_art_nouveau()

```

###### **figure 4.** *The correlation matrix reveals linear relationships between sets of variables. The data shows an expected high correlation between 'Month' and 'Week' (0.899), moderate correlation between the 'Rolling 4-week Average' and 'Lag1' (0.589), and a relatively low correlation between 'Lag2' and 'Lag1' (0.147). The absence of perfect correlation between predictors is essential for avoiding multicollinearity, a common challenge in multiple linear regression that could confound result interpretation*

\
\
\
<br>

### Splitting the Data

With the variables now set, it's time to divide the data for training and testing of the predictive model. Ideally, the previous year would serve as a training ground, providing a foundation for testing the model. However, due the necessity of maintaining the temporal order, I segmented the data by inventory type, 75% into training to test on the remaining 25%.

```{r message=FALSE, warning=FALSE}
# create a list of all items to randomly select
set.seed(1985)
items_sample_list <-  unique(sai$item_no) %>% 
  sample(366)
# 75% of the data


# splitting the inventory by item type to keep the temporal order

is_train <-  sai %>% 
  filter(item_no %in% items_sample_list) %>% 
  dplyr::select(
    item_no,
    week,
    month,
    lag1,
    lag2,
    rolling_4wk_avg,
    best_seller, 
    sold) #not sure why i had to include the dplyr package


is_test<-  sai %>% 
  filter(!(item_no %in% items_sample_list)) %>% 
  dplyr::select(
    item_no,
    week,
    month,
    lag1,
    lag2,
    rolling_4wk_avg,
    best_seller, 
    sold) #not sure why i had to include the dplyr package


## story telling purposes only
# https://r-graph-gallery.com/piechart-ggplot2.html
piecha <- data.frame(
  split = c("train", "test"),
  percentage = c(0.75, 0.25)
)

piecha <- piecha %>%
  arrange(desc(split)) %>%
  mutate(cumulative_percentage = cumsum(percentage) - percentage / 2,
         label = paste(split, ": ", scales::percent(percentage)))

# do you like fancy pie?
ggplot(piecha , aes(x="", y=percentage, fill=split, label = label)) +
  geom_bar(stat="identity", width=2, color="white", alpha=0.6) +
  coord_polar("y", start=0) +
  scale_fill_manual(values=c("#68576D", "#DF5875"), guide = "none") +
  theme_art_nouveau() +
  geom_text(aes(y = cumulative_percentage), color = "white", size = 7, face = "bold") +
  theme(axis.title = element_blank(),
        axis.text = element_blank()) +
  labs(title = "The Data Split",
              caption = "figure 5") +
  theme(plot.title = element_text(hjust = 0.5))
```

\
\
\
<br>

## **Model Development**

### Feature Selection

The next step of the project involves selecting the best model using stepwise selection. It's the iterative process of adding and removing predictors based on their statistical significance. However, it should be used with caution. The inclusion of irrelevant variables can lead to unnecessary complexity in the resulting model and can inflate the variance of prediction errors.

```{r message=FALSE, warning=FALSE}

# https://youtu.be/IScjygOnO0w
# forward Stepwise Selection
# nvmax = 8 specifies the maximum number of predictors to incorporate in the model.
is_mod <- regsubsets(sold ~ week + month + lag1 + lag2 + rolling_4wk_avg + best_seller,
                       data = is_test, nbest = 2, method = "exhaustive")
is_mod_summary_mx <- with(summary(is_mod), data.frame(rsq,adjr2, cp, rss, outmat))

is_mod_summary_mx$predictors <- c(1,1,2,2,3,3,4,4,5,5,6) # for graphing
is_mod_summary_mx$model <- c('a','b','a','b','a','b','a','b','a', 'b', 'a') # for graphing

# update this too  look more like my other graphs
#https://posit.co/blog/great-looking-tables-gt-0-2/# turn results into a pretty table
# is_mod_summary_mx

is_mod_summary_mx %>% 
  mutate(rsq = round(rsq, 1), 
         adjr2 = round(adjr2, 1), 
         cp = scales::comma(round(cp, 1)),
         rss = scales::comma(rss)) %>%
  rename(`4WK ROLLING AVG` = rolling_4wk_avg, `BEST SELLER` = best_seller) %>% 
  select(predictors, model, everything()) %>%
  gt(rowname_col = "predictors") %>%
  tab_header(title = md("Model Comparison")) %>% 
    tab_options(
    grand_summary_row.background.color = "#D35C9E",
    heading.background.color = "#EFFBFC",
    column_labels.background.color = "#EFFBFC",
    stub.background.color = "#EFFBFC",
    table.font.color = "#323232",
    stub.border.style = "dashed",
    stub.border.width = "1px",
    table.width = "60%") %>%
    tab_spanner(
    label = "predictors ",
    columns = c(week, month, lag1, lag2, `4WK ROLLING AVG`, `BEST SELLER`)) %>%
 
  cols_align(align = "center", columns = cp) %>% 
  cols_align(align = "center", columns = rss) %>%
  opt_all_caps() %>% 
  opt_table_font(
    font = google_font(name = "EB Garamond")) %>% 
  tab_source_note(source_note = md("table 1"))
```

###### **table 1.** *the stepwise selection determines the best 2 models based on several goodness-of-fit statistics. Table 1. is the comparison summary of the top two performing models for the number of predictors. Each row represents a model configuration, specified by the model type (a or b) and the predictor variables (week, month, lag1, lag2, 4-week rolling average, best seller). The table displays the coefficient of determination (R-square), the adjusted R-square, the Mallows' Cp, and the residual sum of squares (RSS). An asterisk* (\*)*in the predictors' columns indicates the predictor was included in the model."*
\
<br>

**interpretations**:
RSQ, ADJR2, CP, RSS; these are measures for evaluating and comparing the goodness of fit. These metrics should not be used blindly but together in the context with other measures and domain knowledge.

### **RSQ (R-squared)** proportion of explained variance:
Higher R-squared values explains the variability in the outcome. Based on table 2., models 3a, 4a, 4b, 5a, 5b, and 6a all have the highest R-squared values of 0.6, indicating that these models explain a significant portion of the variance in the outcome variable.
\

### **ADJR2 (Adjusted R-squared)** tradeoff between model complexity and goodness of fit:
The adjusted R-squared is a modified version of R-squared, it incorporates the model's degrees of freedom. The ADJR2 value only increases if the new variable improves the model more than expected by chance, Table 2. shows that models 3a, 4a, 4b, 5a, 5b, and 6a all have the highest adjusted R-squared values of 0.6, showing that these models balance model complexity and goodness-of-fit reasonably well.
\
### **Cp (Mallows' Cp)** bias or the prediction error:
the model will not fit the data very well if the predictions are way off which leads to large residuals. Table 2. shows  model 5a has the lowest Cp. A Cp value close to the total number of predictors suggests that the model is well-fitted. Lower values indicate a better model fit.

\
### **RSS (Residual Sum of Squares)** total prediction error: When the RSS is high, it means the predictions are far off from the actual values, which results in large residuals.

Model 6a has the lowest RSS value of 218,998,262. This indicates that among all the models, model 6a's predictions are closest to the actual values, leading to smaller residuals.
\

### Model Evaluation

```{r fig.height=4, fig.width=10}

gcolors <- c("#68576D", "#DF5875")

is_mod_summary_mx %>%
  pivot_longer(c(adjr2, cp), names_to = "metric", values_to = "value") %>%
  ggplot(aes(x = predictors, y = value, color = model)) +
  geom_line(show.legend = FALSE, size = 1, alpha = .5 ) +
    geom_point(size = 4, alpha = 0.5)+
  facet_wrap(~metric, scales = "free") +
  scale_x_continuous(breaks = 1:6) +
  scale_color_manual(values = gcolors) +
  labs(x = "predictors", y = "metric", title = " Models by Number of Predictors, ADJR2 and CP",
       caption =  " figure 6") +
  theme_art_nouveau() +
  theme(
    strip.text = element_text(size = 20, face = "bold"), 
    axis.title = element_text(size = 20, face = "bold")   
  )
#https://www.youtube.com/watch?v=-BR4WElPIXg
```
###### **Figure 6.** *plots the relationship between the number of predictors, adjusted R-squared (ADJR2), and Mallows' Cp (Cp) for models in table 2. As expected,  an increase in the ADJR2 and a decrease in the CP value with an increasing number of predictors, highlighting an improved model fit. However, the benefits appear to taper off after the inclusion of four predictors. This potentially indicates the onset of overfitting beyond this point, suggesting that a model with four predictors could strike a balance between complexity and predictive power.*

## Maximum likelihood and AIC (Akaike Information Criterion)
To avoid overfitting, I want to take my models through one more level of scrutiny before selecting the best predictors. The AIC, n

The Lower AIC values indicate a better-fit model, and a model with a delta-AIC (the difference between the two AIC values being compared) of more than -2 is considered significantly better than the model it is being compared to.


```{r}
#https://emilhvitfeldt.github.io/ISLR-tidymodels-labs/03-linear-regression.html

# tidy model way

 specs <- linear_reg() %>%
  set_mode("regression") %>%
  set_engine("lm")

lm_3 <- specs %>%
  fit(sold~ lag1 + lag2 + rolling_4wk_avg , data = is_train)

lm_4 <- specs %>%
  fit(sold~ month + lag1 + lag2 + rolling_4wk_avg , data = is_train)

lm_5 <- specs %>%
  fit(sold~ month + week + lag1 + lag2 + rolling_4wk_avg , data = is_train)

mod_compare <-  rbind(glance(lm_3),glance(lm_4),glance(lm_5))
```

```{r}

#
mod_compare %>% 
 gt(rowname_col = "df") %>%
  tab_header(title = md("model comparison")) %>% 
  cols_align(align = "left") %>% 
    tab_options(
    grand_summary_row.background.color = "#D35C9E",
    heading.background.color = "#EFFBFC",
    column_labels.background.color = "#EFFBFC",
    stub.background.color = "#EFFBFC",
    table.font.color = "#323232",
    stub.border.style = "dashed",
    stub.border.width = "1px",
    table.width = "60%"
  ) %>%
  opt_all_caps() 

 # caption = table 2

# 
# is_mod_summary_mx %>% 
#   mutate(rsq = round(rsq, 1), 
#          adjr2 = round(adjr2, 1), 
#          cp = scales::comma(round(cp, 1)),
#          rss = scales::comma(rss)) %>%
#   rename(`4WK ROLLING AVG` = rolling_4wk_avg, `BEST SELLER` = best_seller) %>% 
#   select(predictors, model, everything()) %>%
#   gt(rowname_col = "predictors") %>%
#   tab_header(title = md("Model Comparison")) %>% 
#     tab_options(
#     grand_summary_row.background.color = "#D35C9E",
#     heading.background.color = "#EFFBFC",
#     column_labels.background.color = "#EFFBFC",
#     stub.background.color = "#EFFBFC",
#     table.font.color = "#323232",
#     stub.border.style = "dashed",
#     stub.border.width = "1px",
#     table.width = "60%") %>%
#     tab_spanner(
#     label = "predictors ",
#     columns = c(week, month, lag1, lag2, `4WK ROLLING AVG`, `BEST SELLER`)) %>%
#  
#   cols_align(align = "center", columns = cp) %>% 
#   cols_align(align = "center", columns = rss) %>%
#   opt_all_caps() %>% 
#   opt_table_font(
#     font = google_font(name = "EB Garamond"))
# 






```

###### **table 2. explain this table**

interpret the results

## **Analytics and Evaluation**

```{r}
train_precict <- bind_cols(
  predict(lm_3, new_data = is_train, type = "conf_int"),
  is_train
) %>%
  select(item_no, week, sold, .pred_lower, .pred_upper)

# graph the results
```

###### **Figure 7. explain this graph**

compare the train model against the test

```{r}
lm_3 <- specs %>%
  fit(sold~ lag1 + lag2 + rolling_4wk_avg , data = is_train)

lm_3t <- specs %>%
  fit(sold~ lag1 + lag2 + rolling_4wk_avg , data = is_test)

tt_compare <-  rbind(glance(lm_3),glance(lm_3t))
tt_compare$mod <-  c("train", "test")

# create a visualization
```

```{r}
actual_predicted <- bind_cols(
  predict(lm_3t, new_data = is_train, type = "conf_int"),
  is_train
) %>%
  select(item_no, week, sold, .pred_lower, .pred_upper)
```

###### **Figure 8. explain this graph**

### Conclusion

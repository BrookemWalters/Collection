[
  {
    "objectID": "about_me.html",
    "href": "about_me.html",
    "title": "Brooke Walters",
    "section": "",
    "text": "Bio\nWelcome to my portfolio! I’m Brooke, and I help companies find the hidden stories living in their data. I am fascinated by digital anthropology, machine learning, and the latest developments in AI.\nI’ve been fortunate enough to work in some impressive organizations, from Fortune 100 companies to non-profits of all sizes. Currently, I’m working in the healthcare sector while pursuing my master’s in data science and analytics. Work is just one piece of the puzzle, and I believe it’s crucial to find joy in what you do; for me, that’s advanced analytics."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Portfolio",
    "section": "",
    "text": "---\nbuilding this site over the course of the summer 2023 session of Stat 631"
  },
  {
    "objectID": "sections/glmv1/pi.html",
    "href": "sections/glmv1/pi.html",
    "title": "inventory prediction",
    "section": "",
    "text": "Code\n# import data\nsai &lt;-  read.csv(\"inventory.csv\")\nDRAFT DRAFT OUTLINE"
  },
  {
    "objectID": "sections/glmv1/besti.html",
    "href": "sections/glmv1/besti.html",
    "title": "best_item",
    "section": "",
    "text": "Import Libraries\n\nif (!require(\"pacman\")) {\n  install.packages(\"pacman\")\n  library(\"pacman\")\n}\n\nLoading required package: pacman\n\n# Use p_load() to install (if necessary) and load  packages\npacman::p_load(tidyverse,\n               tidymodels,\n               leaps,\n               corrplot,\n               MMWRweek) # for subset selection\n\n\nsetwd(\"~/gvsu/summer 23/stat 631/Collection/sections/glmv1\")\nai &lt;-  read.csv(\"inventory.csv\")\n\n\n# total yearly sales for each item\ninventory_sum &lt;- ai %&gt;%\n  group_by(item_no) %&gt;%\n  summarise(year_total = sum(sold), .groups = \"drop\")\n\n# Join this back to the original data frame\nai &lt;- left_join(ai, inventory_sum, by = c(\"item_no\"))\n\n\n# Calculate the top 10% threshold\ntop_10_threshold &lt;- quantile(ai$year_total, 0.9)\n\n# Create the 'hv_product' column\nai &lt;- ai %&gt;% \n  mutate(best_seller = ifelse(year_total &gt;= top_10_threshold, 1, 0))\n\n\nai$year &lt;-  2023\nai &lt;- ai %&gt;%\n  mutate(MMWRyear = ifelse(week == 0, year - 1, ifelse(week == 53, year + 1, year)),\n         week = ifelse(week == 0, 52, ifelse(week == 53, 1, week)))\n\nai$date &lt;- MMWRweek::MMWRweek2Date(ai$MMWRyear, ai$week)\nai$month &lt;- month(ai$date, label = TRUE)\nai$month &lt;- month(ai$date)\n\n\nai &lt;-  ai %&gt;% \n  select(\"year\" = MMWRyear,\n         item_no,\n         month,\n         week,\n         best_seller,\n         sold)\n\nhttps://www.youtube.com/watch?v=Kn3llTjYS5E\n\nai &lt;- ai %&gt;%\n  mutate(lag1 = lag(sold, 1),\n         lag2 = lag(sold, 2))\n\n\nai &lt;-  ai %&gt;% \n  filter(item_no == \"A510004\")\n\n\nai &lt;- ai %&gt;% \n  mutate(moving_avg_4weeks = zoo::rollmean(sold, k = 4, fill = NA, align = \"right\"))\n\n\nai &lt;-  ai %&gt;% \n  select(-item_no)\n\n\nset.seed(3746)\n# forward Stepwise Selection\n# nvmax = 8 specifies the maximum number of predictors to incorporate in the model.\nai_mod &lt;- regsubsets(sold ~  .,\n                       data = ai, nbest = 1, method = \"exhaustive\")\n\nWarning in leaps.setup(x, y, wt = wt, nbest = nbest, nvmax = nvmax, force.in =\nforce.in, : 1 linear dependencies found\n\n\nReordering variables and trying again:\n\nai_mod_summary_mx &lt;- with(summary(ai_mod), data.frame(rsq,adjr2, cp, rss, outmat))\nai_mod_summary &lt;- ai_mod_summary_mx %&gt;% \n  select(rsq,adjr2, cp, rss)\n\n\nai_mod_summary\n\n               rsq     adjr2        cp       rss\n1  ( 1 ) 0.1450253 0.1275769 28.840405 146883607\n2  ( 1 ) 0.3873576 0.3618308  9.344357 105251221\n3  ( 1 ) 0.4972487 0.4651582  1.596485  86372067\n4  ( 1 ) 0.4981877 0.4545519  3.513187  86210740\n5  ( 1 ) 0.4986518 0.4429465  5.472016  86131003\n6  ( 1 ) 0.5152464 0.4491436  6.000000  83280082\n\n\n\nrsq &lt;- ai_mod_summary$rsq\n\nnum_predictors &lt;- 1:length(rsq)\n\nai_mod_summary %&gt;% \nggplot( aes(x = num_predictors, y = rsq)) +\n  geom_line() +\n  geom_point() +\n  scale_x_continuous(breaks = 1:9)\n\n\n\n\n\nmod1 &lt;-  lm(sold~ year + month + week + lag1 + lag2 + moving_avg_4weeks, data = ai)\nsummary(mod1)\n\n\nCall:\nlm(formula = sold ~ year + month + week + lag1 + lag2 + moving_avg_4weeks, \n    data = ai)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2550.69  -816.62    -3.24   681.07  3156.49 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       -2.243e+07  1.773e+07  -1.265 0.212476    \nyear               1.109e+04  8.764e+03   1.265 0.212474    \nmonth             -9.466e+02  7.638e+02  -1.239 0.221778    \nweek               2.175e+02  1.772e+02   1.227 0.226238    \nlag1              -3.685e-01  1.187e-01  -3.104 0.003329 ** \nlag2              -5.547e-01  1.297e-01  -4.276 0.000101 ***\nmoving_avg_4weeks  1.978e+00  3.320e-01   5.958 3.89e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1376 on 44 degrees of freedom\n  (3 observations deleted due to missingness)\nMultiple R-squared:  0.5152,    Adjusted R-squared:  0.4491 \nF-statistic: 7.795 on 6 and 44 DF,  p-value: 9.59e-06\n\nmod1_aic &lt;- AIC(mod1)\n\n\nmod2 &lt;-  lm(sold~ year + month + lag1 + lag2 + moving_avg_4weeks, data = ai)\nsummary(mod2)\n\n\nCall:\nlm(formula = sold ~ year + month + lag1 + lag2 + moving_avg_4weeks, \n    data = ai)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2660.41  -796.23   -42.27   678.06  3121.75 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       -9.777e+05  2.982e+06  -0.328  0.74451    \nyear               4.834e+02  1.474e+03   0.328  0.74449    \nmonth             -1.195e+01  5.857e+01  -0.204  0.83919    \nlag1              -3.735e-01  1.193e-01  -3.132  0.00305 ** \nlag2              -6.205e-01  1.188e-01  -5.225 4.34e-06 ***\nmoving_avg_4weeks  1.990e+00  3.337e-01   5.964 3.53e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1383 on 45 degrees of freedom\n  (3 observations deleted due to missingness)\nMultiple R-squared:  0.4987,    Adjusted R-squared:  0.4429 \nF-statistic: 8.952 on 5 and 45 DF,  p-value: 5.85e-06\n\nmod2_aic &lt;- AIC(mod2)\n\n\nmod3 &lt;-  lm(sold~ year + lag1 + lag2 + moving_avg_4weeks, data = ai)\nsummary(mod3)\n\n\nCall:\nlm(formula = sold ~ year + lag1 + lag2 + moving_avg_4weeks, data = ai)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2666.08  -783.55   -12.54   682.31  3128.39 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       -8.448e+05  2.879e+06  -0.293  0.77055    \nyear               4.176e+02  1.423e+03   0.293  0.77054    \nlag1              -3.745e-01  1.179e-01  -3.176  0.00267 ** \nlag2              -6.202e-01  1.175e-01  -5.278 3.43e-06 ***\nmoving_avg_4weeks  1.989e+00  3.302e-01   6.025 2.65e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1369 on 46 degrees of freedom\n  (3 observations deleted due to missingness)\nMultiple R-squared:  0.4982,    Adjusted R-squared:  0.4546 \nF-statistic: 11.42 on 4 and 46 DF,  p-value: 1.614e-06\n\nmod3_aic &lt;- AIC(mod3)\n\n\nnext up, how can u split the data without changing the weeks?\nI can split it up by inventory??\nadd back in the best sellers and compare AICs?"
  },
  {
    "objectID": "sections/references/references.html#resources",
    "href": "sections/references/references.html#resources",
    "title": "References",
    "section": "Resources",
    "text": "Resources\n\nNHS-R Community, “Plotting interactive visualizations with Plotly in R”\nPosit, “Great Looking Tables: gt (v0.2)”\nAlexandre Morin-Chassé, “The Stellar Chart: An Elegant Alternative to Radar Charts”\nMatt Bartholomew, “Making a Better Map Part 2: ggplot and Tigris”"
  },
  {
    "objectID": "sections/references/references.html#image-credits",
    "href": "sections/references/references.html#image-credits",
    "title": "References",
    "section": "Image Credits",
    "text": "Image Credits\n\nArtsy\nGood Studio\nOskar Reschke"
  },
  {
    "objectID": "sections/reflections/goals.html",
    "href": "sections/reflections/goals.html",
    "title": "Goals",
    "section": "",
    "text": "Goals\n\nMission:\n\nDesign a portfolio showcasing key skills sought after in the data science industry, such as SQL, A/B testing, and classification models.\nBridge the gap between my current resume and market demand, while demonstrating STAT 631 course objectives.\n\n\n\n\n\nProject Objectives:\n\nInvestigate the interplay between a municipality’s walkability index (proximity and density of amenities) and various socio-economic factors within the Greater Grand Rapids Area by leveraging statistical modeling and data analysis techniques.\n\n\n\n\nConstruct a SQL database integrating time-series data from reputable sources including but not limited to:\n\nNational Center for Health Statistics\nUS Bureau of Labor Statistics\nWalkability Scores\n\n\n\n\n\nBuild and evaluate three different predictive models about the impact of a city’s walkability on socio-economic parameters. Potential relationships to explore:\n\nIndustries Growth Type and Walkability\nWalkability Scores and Average Life Expectancy\nIncome Disparity and Walkability Scores\n\n\n\n\n\nDemonstrate data storytelling in a ‘scannable’ format guided by UX design principles. The focus will be on translating complex statistical results into business language that appeals to a corporate audience. The portfolio will be designed using Quarto and hosted on GitHub pages."
  },
  {
    "objectID": "sections/reflections/knowledge/knowledge.html",
    "href": "sections/reflections/knowledge/knowledge.html",
    "title": "Current Level of Understanding",
    "section": "",
    "text": "Figure above represents aggregation of my self-assessed level of understanding on concepts presented in “Introduction to Statistical Learning” and “Data Feminism”."
  },
  {
    "objectID": "sections/reflections/knowledge/knowledge.html#explanation-of-assessment",
    "href": "sections/reflections/knowledge/knowledge.html#explanation-of-assessment",
    "title": "Current Level of Understanding",
    "section": "Explanation of Assessment",
    "text": "Explanation of Assessment\n\nProficient Areas:\nI marked items here as proficient because I have had repeated academic exposure and application of these concepts.\n\nLinear regression:\n\nI have a basic understanding of the underlying concepts, assumptions, and techniques involved in linear regression. I have completed assignments and projects that required implementing linear regression and analyzing the results.\n\nFor example, in my final project last semester in CIS 631, I conducted an analysis using multiple linear regression. The project focused on evaluating the relationship between crime rates and the presence of cannabis dispensaries while controlling for socioeconomic factors. I collected relevant data, preprocessed it, and applied multiple linear regression to model the relationship. I carefully interpreted the results and drew meaningful conclusions based on statistical significance and the summary of my model’s output.\n\nOverall, my proficiency in linear regression is based on a combination of theoretical knowledge gained through coursework, practical application in projects and assignments, and the ability to critically analyze and interpret the results obtained.\n\n\n\n\nAware Areas:\nI marked these items as aware because I am familiar with the terms,  but have limited practice applying the techniques.\n\nData Feminism:\n\nMy awareness here derives from a combination of the literature and media I consume, along with principles discussed in PSM 650 – “Ethical and professionalism” and CIS 631– “Data Mining”.\nI’d like to note that I bought the audiobook version of Data Feminism over Memorial Day. I am finding it easier to digest the philosophy presented in that format. \n\nClassification:\n\nI am familiar with the terms in classification and the types of ways we can categorize data as ordinal, nominal and categorical. I’ve used K-nearest neighbors algorithms, and decision trees in CIS 500 – “Fundamentals of Software Practice.”  I recognize I need to more exposure and practice here.\n\nLinear Model Selection and Regularization:\n\nLooking over the section titles of this ISL chapter, I see I have had some practice with dimension reduction using principle component analysis but have not taken the opportunity to apply this methodology outside of a lab exercise.\nAdditionally, as part of your Stat 518 – Statistical Computing and Graphics with R, I deployed bootstrapping methodology in the final project.\n\nMultiple Testing:\n\nI have had exposure to these concepts both professionally, when communicating market research and survey results and academically, particularly in Stat 216. I feel most comfortable with hypothesis testing , understanding type 1 and type 2 errors, and evaluating and explaining p-values.\nI marked this one as aware verses proficient because I feel I need more reinforcement learning in this area."
  },
  {
    "objectID": "sections/reflections/knowledge/knowledge.html#detailed-self-assessment",
    "href": "sections/reflections/knowledge/knowledge.html#detailed-self-assessment",
    "title": "Current Level of Understanding",
    "section": "Detailed Self-Assessment",
    "text": "Detailed Self-Assessment\n\n\n\n\n\n\n  \n    \n      Course Concepts\n    \n    \n    \n      \n      Section\n      Section Title\n      Self Assessment1\n    \n  \n  \n    ISL\n3.2\nMultiple Linear Regression\n3.0\n    ISL\n3.3\nOther Considerations in the Regression Model\n3.0\n    ISL\n3.4\nThe Marketing Plan\n3.0\n    ISL\n3.5\nComparison of Linear Regression with K-Nearest Neighbors\n2.5\n    ISL\n4.1\nAn Overview of Classification\n3.0\n    ISL\n4.2\nWhy Not Linear Regression?\n3.0\n    ISL\n4.3\nLogistic Regression\n2.0\n    ISL\n4.4\nGenerative Models for Classification\n1.5\n    ISL\n4.5\nA Comparison of Classification Methods\n2.0\n    ISL\n4.6\nGeneralized Linear Models\n2.0\n    ISL\n5.1\nCross-Validation\n2.0\n    ISL\n5.2\nThe Bootstrap\n2.5\n    ISL\n6.1\nSubset Selection\n2.0\n    ISL\n6.2\nShrinkage Methods\n2.0\n    ISL\n6.3\nDimension Reduction Methods\n2.5\n    ISL\n6.4\nConsiderations in High Dimensions\n2.0\n    ISL\n13.1\nA Quick Review of Hypothesis Testing\n3.0\n    ISL\n13.2\nThe Challenge of Multiple Testing\n2.0\n    ISL\n13.3\nThe Family-Wise Error Rate\n2.0\n    ISL\n13.4\nThe False Discovery Rate\n2.0\n    ISL\n13.5\n A Re-Sampling Approach to p-Values and False Discovery Rates\n2.0\n    DF\nn/a\nThe Power Chapter\n2.0\n    DF\nn/a\nCollect, Analyze, Imagine, Teach\n2.0\n    DF\nn/a\nOn Rational, Scientific, Objective Viewpoints from Mythical, Imaginary, Impossible Standpoints\n2.0\n    DF\nn/a\nWhat Gets Counted Counts\n2.0\n    DF\nn/a\nUnicorns, Janitors, Ninjas, Wizards, and Rock Stars\n2.0\n    DF\nn/a\nThe Numbers Dont Speak for Themselves\n2.0\n    DF\nn/a\nShow Your Work\n2.0\n  \n  \n  \n    \n      1 Proficiency level ranges from 1 (low) to 3 (high)."
  },
  {
    "objectID": "sections/reflections/pitch.html#introduction",
    "href": "sections/reflections/pitch.html#introduction",
    "title": "Portfolio Pitch",
    "section": "Introduction",
    "text": "Introduction\nThe perceived ease of my communications degree was a recurring punchline in my repertoire of self-deprecating jokes. Now studying a quantitative field, I find myself navigating a dichotomy of self-doubt and relentless drive. However, it’s time to lean into my liberal arts roots. This complementary combination of skills is my differentiator and is crucial in the data science landscape. After all, what value are analytical insights if they cannot be communicated effectively?\nFor this portfolio I will flex my marketing muscle to demonstrate practical application of statistical modeling and regression in a clear, concise, and visually engaging way."
  },
  {
    "objectID": "sections/reflections/pitch.html#elements",
    "href": "sections/reflections/pitch.html#elements",
    "title": "Portfolio Pitch",
    "section": "Elements",
    "text": "Elements\nMy portfolio’s “brand theme” is inspired by femininity, philosophy and the art nouveau movement of the early 20th century. Using these styling elements, my goal is to build several mini-interconnected projects utilizing R with a dash of MySQL .\nThe assigned reflections are incorporated into this site to exercise and refine my writing skills alongside deepening my understanding of statistical modeling. I’ve chosen to use them as a platform to experiment with less conventional data visualizations, unbound by the professional constraints of my day job; a refreshing departure from the effective, yet monotonous, bar charts and tables that typically dominate the market intelligence reports I produce."
  },
  {
    "objectID": "sections/reflections/pitch.html#research-question",
    "href": "sections/reflections/pitch.html#research-question",
    "title": "Portfolio Pitch",
    "section": "Research Question",
    "text": "Research Question\nAs for a topic, I’m still mulling that one over. I am inspired by the intersectionality of my hometown Grand Rapids’ economic growth, the housing crisis, and the hellscape that is suburban sprawl.\n\nIdea #1\nWhat Makes a “Good” City? | Market Analysis and Future Predictions for the Greater Grand Rapids, Mi Area\npotential data sources:\n\nCounty Health Rankings\nWalkability Scores\nNational Center for Health Statistics"
  },
  {
    "objectID": "sections/reflections/reflections_main.html",
    "href": "sections/reflections/reflections_main.html",
    "title": "collection of reflections",
    "section": "",
    "text": "click image to read the reflection"
  },
  {
    "objectID": "sections/glmv1/pi.html#clear-description-of-data",
    "href": "sections/glmv1/pi.html#clear-description-of-data",
    "title": "inventory prediction",
    "section": "Clear Description of Data:",
    "text": "Clear Description of Data:\nStart by providing a comprehensive summary of the dataset. This includes source information, timeframe, and the main purpose for collection. Each variable should be properly named and described. This could be presented in the form of a data dictionary.\n[insert]"
  },
  {
    "objectID": "sections/glmv1/pi.html#statistical-summary",
    "href": "sections/glmv1/pi.html#statistical-summary",
    "title": "inventory prediction",
    "section": "Statistical Summary:",
    "text": "Statistical Summary:\nProvide a statistical summary of the variables. This includes measures of central tendency (mean, median, mode), dispersion (range, standard deviation, variance), and distribution properties (skewness, kurtosis). This could be supplemented with summary tables and histograms.\n[insert]"
  },
  {
    "objectID": "sections/glmv1/pi.html#visual-exploration",
    "href": "sections/glmv1/pi.html#visual-exploration",
    "title": "inventory prediction",
    "section": "Visual Exploration:",
    "text": "Visual Exploration:\nUse visualizations to give an overview of the data. Histograms, box plots, scatter plots, or correlation heatmaps can be used to show relationships and trends within the data..\n[insert]"
  },
  {
    "objectID": "sections/glmv1/pi.html#initial-insights",
    "href": "sections/glmv1/pi.html#initial-insights",
    "title": "inventory prediction",
    "section": "Initial Insights:",
    "text": "Initial Insights:\nStart by providing a comprehensive summary of the dataset. This includes source information, timeframe, and the main purpose for collection. Each variable should be properly named and described. This could be presented in the form of a data dictionary.\n[insert]"
  },
  {
    "objectID": "sections/glmv1/pi.html#model-evaluation",
    "href": "sections/glmv1/pi.html#model-evaluation",
    "title": "inventory prediction",
    "section": "Model Evaluation",
    "text": "Model Evaluation\nfind the best model using stepwise selection find the 2 best methods for every size predictors\n\n\nCode\n# https://youtu.be/IScjygOnO0w\nset.seed(3746)\n# forward Stepwise Selection\n# nvmax = 8 specifies the maximum number of predictors to incorporate in the model.\na5i_mod &lt;- regsubsets(sold ~  .,\n                       data = a5i, nbest = 2, method = \"exhaustive\")\na5i_mod_summary_mx &lt;- with(summary(a5i_mod), data.frame(rsq,adjr2, cp, rss, outmat))\n\na5i_mod_summary_mx$predictors_cnt &lt;- c(1,1,2,2,3,3,4,4,5) # for graphing\na5i_mod_summary_mx$model &lt;- c('a','b','a','b','a','b','a','b','a') # for graphing\n\n# insert the num_predictors column to a5i_mod_summary_mx\na5i_mod_summary_mx \n\n\n                rsq     adjr2        cp       rss month week lag2 lag1\n1  ( 1 ) 0.14502533 0.1275769 31.959462 146883607                     \n1  ( 2 ) 0.09746449 0.0790454 36.351847 155054501               *     \n2  ( 1 ) 0.38735758 0.3618308 11.579355 105251221               *     \n2  ( 2 ) 0.18497440 0.1510150 30.270046 140020406                    *\n3  ( 1 ) 0.49724866 0.4651582  3.430586  86372067               *    *\n3  ( 2 ) 0.38901570 0.3500167 13.426223 104966358     *         *     \n4  ( 1 ) 0.49802651 0.4543766  5.358750  86238434     *         *    *\n4  ( 2 ) 0.49758788 0.4538999  5.399259  86313791          *    *    *\n5  ( 1 ) 0.51273908 0.4585990  6.000000  83710833     *    *    *    *\n         moving_avg_4weeks predictors_cnt model\n1  ( 1 )                 *              1     a\n1  ( 2 )                                1     b\n2  ( 1 )                 *              2     a\n2  ( 2 )                 *              2     b\n3  ( 1 )                 *              3     a\n3  ( 2 )                 *              3     b\n4  ( 1 )                 *              4     a\n4  ( 2 )                 *              4     b\n5  ( 1 )                 *              5     a\n\n\n\n\nCode\na5i_mod_summary_mx %&gt;% \n  pivot_longer(c(adjr2, cp), names_to = \"metric\", values_to = \"value\") %&gt;%\n  ggplot(aes(x = predictors_cnt, y = value, color = model)) +\n  geom_line(show.legend = F) +\n  geom_point() +\n  facet_wrap(~metric, scales = \"free\") +\n  scale_x_continuous(breaks = 1:5)"
  }
]
[
  {
    "objectID": "about_me.html",
    "href": "about_me.html",
    "title": "Brooke Walters",
    "section": "",
    "text": "Bio\nWelcome to my portfolio! I’m Brooke, and I help companies find the hidden stories living in their data. I am fascinated by digital anthropology, machine learning, and the latest developments in AI.\nI’ve been fortunate enough to work in some impressive organizations, from Fortune 100 companies to non-profits of all sizes. Currently, I’m working in the healthcare sector while pursuing my master’s in data science and analytics. Work is just one piece of the puzzle, and I believe it’s crucial to find joy in what you do; for me, that’s advanced analytics."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Brooke Walters",
    "section": "",
    "text": "Bio\nWelcome to my portfolio! I’m Brooke, and I help companies find the hidden stories living in their data. I am fascinated by digital anthropology, machine learning, and the latest developments in AI.\nI’ve been fortunate enough to work in some impressive organizations, from Fortune 100 companies to non-profits of all sizes. Currently, I’m working in the healthcare sector while pursuing my master’s in data science and analytics. Work is just one piece of the puzzle, and I believe it’s crucial to find joy in what you do; for me, that’s advanced analytics."
  },
  {
    "objectID": "sections/glmv1/pi.html",
    "href": "sections/glmv1/pi.html",
    "title": "Project Overview",
    "section": "",
    "text": "DRAFT DRAFT LESS OUTLINE MORE WRITING"
  },
  {
    "objectID": "sections/glmv1/besti.html",
    "href": "sections/glmv1/besti.html",
    "title": "best_item",
    "section": "",
    "text": "Import Libraries\n\nif (!require(\"pacman\")) {\n  install.packages(\"pacman\")\n  library(\"pacman\")\n}\n\nLoading required package: pacman\n\n# Use p_load() to install (if necessary) and load  packages\npacman::p_load(tidyverse,\n               tidymodels,\n               leaps,\n               corrplot,\n               MMWRweek) # for subset selection\n\n\nsetwd(\"~/gvsu/summer 23/stat 631/Collection/sections/glmv1\")\nai &lt;-  read.csv(\"inventory.csv\")\n\n\n# total yearly sales for each item\ninventory_sum &lt;- ai %&gt;%\n  group_by(item_no) %&gt;%\n  summarise(year_total = sum(sold), .groups = \"drop\")\n\n# Join this back to the original data frame\nai &lt;- left_join(ai, inventory_sum, by = c(\"item_no\"))\n\n\n# Calculate the top 10% threshold\ntop_10_threshold &lt;- quantile(ai$year_total, 0.9)\n\n# Create the 'hv_product' column\nai &lt;- ai %&gt;% \n  mutate(best_seller = ifelse(year_total &gt;= top_10_threshold, 1, 0))\n\n\nai$year &lt;-  2023\nai &lt;- ai %&gt;%\n  mutate(MMWRyear = ifelse(week == 0, year - 1, ifelse(week == 53, year + 1, year)),\n         week = ifelse(week == 0, 52, ifelse(week == 53, 1, week)))\n\nai$date &lt;- MMWRweek::MMWRweek2Date(ai$MMWRyear, ai$week)\nai$month &lt;- month(ai$date, label = TRUE)\nai$month &lt;- month(ai$date)\n\n\nai &lt;-  ai %&gt;% \n  select(\"year\" = MMWRyear,\n         item_no,\n         month,\n         week,\n         best_seller,\n         sold)\n\nhttps://www.youtube.com/watch?v=Kn3llTjYS5E\n\nai &lt;- ai %&gt;%\n  mutate(lag1 = lag(sold, 1),\n         lag2 = lag(sold, 2))\n\n\nai &lt;-  ai %&gt;% \n  filter(item_no == \"A510004\")\n\n\nai &lt;- ai %&gt;% \n  mutate(moving_avg_4weeks = zoo::rollmean(sold, k = 4, fill = NA, align = \"right\"))\n\n\nai &lt;-  ai %&gt;% \n  select(-item_no)\n\n\nset.seed(3746)\n# forward Stepwise Selection\n# nvmax = 8 specifies the maximum number of predictors to incorporate in the model.\nai_mod &lt;- regsubsets(sold ~  .,\n                       data = ai, nbest = 1, method = \"exhaustive\")\n\nWarning in leaps.setup(x, y, wt = wt, nbest = nbest, nvmax = nvmax, force.in =\nforce.in, : 1 linear dependencies found\n\n\nReordering variables and trying again:\n\nai_mod_summary_mx &lt;- with(summary(ai_mod), data.frame(rsq,adjr2, cp, rss, outmat))\nai_mod_summary &lt;- ai_mod_summary_mx %&gt;% \n  select(rsq,adjr2, cp, rss)\n\n\nai_mod_summary\n\n               rsq     adjr2        cp       rss\n1  ( 1 ) 0.1450253 0.1275769 28.840405 146883607\n2  ( 1 ) 0.3873576 0.3618308  9.344357 105251221\n3  ( 1 ) 0.4972487 0.4651582  1.596485  86372067\n4  ( 1 ) 0.4981877 0.4545519  3.513187  86210740\n5  ( 1 ) 0.4986518 0.4429465  5.472016  86131003\n6  ( 1 ) 0.5152464 0.4491436  6.000000  83280082\n\n\n\nrsq &lt;- ai_mod_summary$rsq\n\nnum_predictors &lt;- 1:length(rsq)\n\nai_mod_summary %&gt;% \nggplot( aes(x = num_predictors, y = rsq)) +\n  geom_line() +\n  geom_point() +\n  scale_x_continuous(breaks = 1:9)\n\n\n\n\n\nmod1 &lt;-  lm(sold~ year + month + week + lag1 + lag2 + moving_avg_4weeks, data = ai)\nsummary(mod1)\n\n\nCall:\nlm(formula = sold ~ year + month + week + lag1 + lag2 + moving_avg_4weeks, \n    data = ai)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2550.69  -816.62    -3.24   681.07  3156.49 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       -2.243e+07  1.773e+07  -1.265 0.212476    \nyear               1.109e+04  8.764e+03   1.265 0.212474    \nmonth             -9.466e+02  7.638e+02  -1.239 0.221778    \nweek               2.175e+02  1.772e+02   1.227 0.226238    \nlag1              -3.685e-01  1.187e-01  -3.104 0.003329 ** \nlag2              -5.547e-01  1.297e-01  -4.276 0.000101 ***\nmoving_avg_4weeks  1.978e+00  3.320e-01   5.958 3.89e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1376 on 44 degrees of freedom\n  (3 observations deleted due to missingness)\nMultiple R-squared:  0.5152,    Adjusted R-squared:  0.4491 \nF-statistic: 7.795 on 6 and 44 DF,  p-value: 9.59e-06\n\nmod1_aic &lt;- AIC(mod1)\n\n\nmod2 &lt;-  lm(sold~ year + month + lag1 + lag2 + moving_avg_4weeks, data = ai)\nsummary(mod2)\n\n\nCall:\nlm(formula = sold ~ year + month + lag1 + lag2 + moving_avg_4weeks, \n    data = ai)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2660.41  -796.23   -42.27   678.06  3121.75 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       -9.777e+05  2.982e+06  -0.328  0.74451    \nyear               4.834e+02  1.474e+03   0.328  0.74449    \nmonth             -1.195e+01  5.857e+01  -0.204  0.83919    \nlag1              -3.735e-01  1.193e-01  -3.132  0.00305 ** \nlag2              -6.205e-01  1.188e-01  -5.225 4.34e-06 ***\nmoving_avg_4weeks  1.990e+00  3.337e-01   5.964 3.53e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1383 on 45 degrees of freedom\n  (3 observations deleted due to missingness)\nMultiple R-squared:  0.4987,    Adjusted R-squared:  0.4429 \nF-statistic: 8.952 on 5 and 45 DF,  p-value: 5.85e-06\n\nmod2_aic &lt;- AIC(mod2)\n\n\nmod3 &lt;-  lm(sold~ year + lag1 + lag2 + moving_avg_4weeks, data = ai)\nsummary(mod3)\n\n\nCall:\nlm(formula = sold ~ year + lag1 + lag2 + moving_avg_4weeks, data = ai)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2666.08  -783.55   -12.54   682.31  3128.39 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       -8.448e+05  2.879e+06  -0.293  0.77055    \nyear               4.176e+02  1.423e+03   0.293  0.77054    \nlag1              -3.745e-01  1.179e-01  -3.176  0.00267 ** \nlag2              -6.202e-01  1.175e-01  -5.278 3.43e-06 ***\nmoving_avg_4weeks  1.989e+00  3.302e-01   6.025 2.65e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1369 on 46 degrees of freedom\n  (3 observations deleted due to missingness)\nMultiple R-squared:  0.4982,    Adjusted R-squared:  0.4546 \nF-statistic: 11.42 on 4 and 46 DF,  p-value: 1.614e-06\n\nmod3_aic &lt;- AIC(mod3)\n\n\nnext up, how can u split the data without changing the weeks?\nI can split it up by inventory??\nadd back in the best sellers and compare AICs?"
  },
  {
    "objectID": "sections/references/references.html#resources",
    "href": "sections/references/references.html#resources",
    "title": "References",
    "section": "Resources",
    "text": "Resources\n\nNHS-R Community, “Plotting interactive visualizations with Plotly in R”\nPosit, “Great Looking Tables: gt (v0.2)”\nAlexandre Morin-Chassé, “The Stellar Chart: An Elegant Alternative to Radar Charts”\nMatt Bartholomew, “Making a Better Map Part 2: ggplot and Tigris”"
  },
  {
    "objectID": "sections/references/references.html#image-credits",
    "href": "sections/references/references.html#image-credits",
    "title": "References",
    "section": "Image Credits",
    "text": "Image Credits\n\nArtsy\nGood Studio\nOskar Reschke"
  },
  {
    "objectID": "sections/reflections/goals.html",
    "href": "sections/reflections/goals.html",
    "title": "Goals",
    "section": "",
    "text": "Goals\n\nMission:\n\nDesign a portfolio showcasing key skills sought after in the data science industry, such as SQL, A/B testing, and classification models.\nBridge the gap between my current resume and market demand, while demonstrating STAT 631 course objectives.\n\n\n\n\n\nProject Objectives:\n\nInvestigate the interplay between a municipality’s walkability index (proximity and density of amenities) and various socio-economic factors within the Greater Grand Rapids Area by leveraging statistical modeling and data analysis techniques.\n\n\n\n\nConstruct a SQL database integrating time-series data from reputable sources including but not limited to:\n\nNational Center for Health Statistics\nUS Bureau of Labor Statistics\nWalkability Scores\n\n\n\n\n\nBuild and evaluate three different predictive models about the impact of a city’s walkability on socio-economic parameters. Potential relationships to explore:\n\nIndustries Growth Type and Walkability\nWalkability Scores and Average Life Expectancy\nIncome Disparity and Walkability Scores\n\n\n\n\n\nDemonstrate data storytelling in a ‘scannable’ format guided by UX design principles. The focus will be on translating complex statistical results into business language that appeals to a corporate audience. The portfolio will be designed using Quarto and hosted on GitHub pages."
  },
  {
    "objectID": "sections/reflections/knowledge/knowledge.html",
    "href": "sections/reflections/knowledge/knowledge.html",
    "title": "Current Level of Understanding",
    "section": "",
    "text": "Figure above represents aggregation of my self-assessed level of understanding on concepts presented in “Introduction to Statistical Learning” and “Data Feminism”."
  },
  {
    "objectID": "sections/reflections/knowledge/knowledge.html#explanation-of-assessment",
    "href": "sections/reflections/knowledge/knowledge.html#explanation-of-assessment",
    "title": "Current Level of Understanding",
    "section": "Explanation of Assessment",
    "text": "Explanation of Assessment\n\nProficient Areas:\nI marked items here as proficient because I have had repeated academic exposure and application of these concepts.\n\nLinear regression:\n\nI have a basic understanding of the underlying concepts, assumptions, and techniques involved in linear regression. I have completed assignments and projects that required implementing linear regression and analyzing the results.\n\nFor example, in my final project last semester in CIS 631, I conducted an analysis using multiple linear regression. The project focused on evaluating the relationship between crime rates and the presence of cannabis dispensaries while controlling for socioeconomic factors. I collected relevant data, preprocessed it, and applied multiple linear regression to model the relationship. I carefully interpreted the results and drew meaningful conclusions based on statistical significance and the summary of my model’s output.\n\nOverall, my proficiency in linear regression is based on a combination of theoretical knowledge gained through coursework, practical application in projects and assignments, and the ability to critically analyze and interpret the results obtained.\n\n\n\n\nAware Areas:\nI marked these items as aware because I am familiar with the terms,  but have limited practice applying the techniques.\n\nData Feminism:\n\nMy awareness here derives from a combination of the literature and media I consume, along with principles discussed in PSM 650 – “Ethical and professionalism” and CIS 631– “Data Mining”.\nI’d like to note that I bought the audiobook version of Data Feminism over Memorial Day. I am finding it easier to digest the philosophy presented in that format. \n\nClassification:\n\nI am familiar with the terms in classification and the types of ways we can categorize data as ordinal, nominal and categorical. I’ve used K-nearest neighbors algorithms, and decision trees in CIS 500 – “Fundamentals of Software Practice.”  I recognize I need to more exposure and practice here.\n\nLinear Model Selection and Regularization:\n\nLooking over the section titles of this ISL chapter, I see I have had some practice with dimension reduction using principle component analysis but have not taken the opportunity to apply this methodology outside of a lab exercise.\nAdditionally, as part of your Stat 518 – Statistical Computing and Graphics with R, I deployed bootstrapping methodology in the final project.\n\nMultiple Testing:\n\nI have had exposure to these concepts both professionally, when communicating market research and survey results and academically, particularly in Stat 216. I feel most comfortable with hypothesis testing , understanding type 1 and type 2 errors, and evaluating and explaining p-values.\nI marked this one as aware verses proficient because I feel I need more reinforcement learning in this area."
  },
  {
    "objectID": "sections/reflections/knowledge/knowledge.html#detailed-self-assessment",
    "href": "sections/reflections/knowledge/knowledge.html#detailed-self-assessment",
    "title": "Current Level of Understanding",
    "section": "Detailed Self-Assessment",
    "text": "Detailed Self-Assessment\n\n\n\n\n\n\n  \n    \n      Course Concepts\n    \n    \n    \n      \n      Section\n      Section Title\n      Self Assessment1\n    \n  \n  \n    ISL\n3.2\nMultiple Linear Regression\n3.0\n    ISL\n3.3\nOther Considerations in the Regression Model\n3.0\n    ISL\n3.4\nThe Marketing Plan\n3.0\n    ISL\n3.5\nComparison of Linear Regression with K-Nearest Neighbors\n2.5\n    ISL\n4.1\nAn Overview of Classification\n3.0\n    ISL\n4.2\nWhy Not Linear Regression?\n3.0\n    ISL\n4.3\nLogistic Regression\n2.0\n    ISL\n4.4\nGenerative Models for Classification\n1.5\n    ISL\n4.5\nA Comparison of Classification Methods\n2.0\n    ISL\n4.6\nGeneralized Linear Models\n2.0\n    ISL\n5.1\nCross-Validation\n2.0\n    ISL\n5.2\nThe Bootstrap\n2.5\n    ISL\n6.1\nSubset Selection\n2.0\n    ISL\n6.2\nShrinkage Methods\n2.0\n    ISL\n6.3\nDimension Reduction Methods\n2.5\n    ISL\n6.4\nConsiderations in High Dimensions\n2.0\n    ISL\n13.1\nA Quick Review of Hypothesis Testing\n3.0\n    ISL\n13.2\nThe Challenge of Multiple Testing\n2.0\n    ISL\n13.3\nThe Family-Wise Error Rate\n2.0\n    ISL\n13.4\nThe False Discovery Rate\n2.0\n    ISL\n13.5\n A Re-Sampling Approach to p-Values and False Discovery Rates\n2.0\n    DF\nn/a\nThe Power Chapter\n2.0\n    DF\nn/a\nCollect, Analyze, Imagine, Teach\n2.0\n    DF\nn/a\nOn Rational, Scientific, Objective Viewpoints from Mythical, Imaginary, Impossible Standpoints\n2.0\n    DF\nn/a\nWhat Gets Counted Counts\n2.0\n    DF\nn/a\nUnicorns, Janitors, Ninjas, Wizards, and Rock Stars\n2.0\n    DF\nn/a\nThe Numbers Dont Speak for Themselves\n2.0\n    DF\nn/a\nShow Your Work\n2.0\n  \n  \n  \n    \n      1 Proficiency level ranges from 1 (low) to 3 (high)."
  },
  {
    "objectID": "sections/reflections/pitch.html#introduction",
    "href": "sections/reflections/pitch.html#introduction",
    "title": "Portfolio Pitch",
    "section": "Introduction",
    "text": "Introduction\nThe perceived ease of my communications degree was a recurring punchline in my repertoire of self-deprecating jokes. Now studying a quantitative field, I find myself navigating a dichotomy of self-doubt and relentless drive. However, it’s time to lean into my liberal arts roots. This complementary combination of skills is my differentiator and is crucial in the data science landscape. After all, what value are analytical insights if they cannot be communicated effectively?\nFor this portfolio I will flex my marketing muscle to demonstrate practical application of statistical modeling and regression in a clear, concise, and visually engaging way."
  },
  {
    "objectID": "sections/reflections/pitch.html#elements",
    "href": "sections/reflections/pitch.html#elements",
    "title": "Portfolio Pitch",
    "section": "Elements",
    "text": "Elements\nMy portfolio’s “brand theme” is inspired by femininity, philosophy and the art nouveau movement of the early 20th century. Using these styling elements, my goal is to build several mini-interconnected projects utilizing R with a dash of MySQL .\nThe assigned reflections are incorporated into this site to exercise and refine my writing skills alongside deepening my understanding of statistical modeling. I’ve chosen to use them as a platform to experiment with less conventional data visualizations, unbound by the professional constraints of my day job; a refreshing departure from the effective, yet monotonous, bar charts and tables that typically dominate the market intelligence reports I produce."
  },
  {
    "objectID": "sections/reflections/pitch.html#research-question",
    "href": "sections/reflections/pitch.html#research-question",
    "title": "Portfolio Pitch",
    "section": "Research Question",
    "text": "Research Question\nAs for a topic, I’m still mulling that one over. I am inspired by the intersectionality of my hometown Grand Rapids’ economic growth, the housing crisis, and the hellscape that is suburban sprawl.\n\nIdea #1\nWhat Makes a “Good” City? | Market Analysis and Future Predictions for the Greater Grand Rapids, Mi Area\npotential data sources:\n\nCounty Health Rankings\nWalkability Scores\nNational Center for Health Statistics"
  },
  {
    "objectID": "sections/reflections/reflections_main.html",
    "href": "sections/reflections/reflections_main.html",
    "title": "collection of reflections",
    "section": "",
    "text": "click image to read the reflection"
  },
  {
    "objectID": "sections/glmv1/pi.html#clear-description-of-data",
    "href": "sections/glmv1/pi.html#clear-description-of-data",
    "title": "inventory prediction",
    "section": "Clear Description of Data:",
    "text": "Clear Description of Data:\nStart by providing a comprehensive summary of the dataset. This includes source information, timeframe, and the main purpose for collection. Each variable should be properly named and described. This could be presented in the form of a data dictionary.\n[insert]"
  },
  {
    "objectID": "sections/glmv1/pi.html#statistical-summary",
    "href": "sections/glmv1/pi.html#statistical-summary",
    "title": "inventory prediction",
    "section": "Statistical Summary:",
    "text": "Statistical Summary:\n\n\nCode\nbox &lt;- sai %&gt;% \n\n  ggplot(aes( x = sold)) +\n  geom_boxplot(fill = \"#7D998F\", color = \"black\", alpha = 0.6) +\n  labs(x = \"Item Number\", y = \"Sales\") +\n  theme_minimal()\n\nbox\n\n\n\n\n\n[insert]\n[insert]"
  },
  {
    "objectID": "sections/glmv1/pi.html#visual-exploration",
    "href": "sections/glmv1/pi.html#visual-exploration",
    "title": "inventory prediction",
    "section": "Visual Exploration:",
    "text": "Visual Exploration:\nUse visualizations to give an overview of the data. Histograms, box plots, scatter plots, or correlation heatmaps can be used to show relationships and trends within the data..\n[insert]"
  },
  {
    "objectID": "sections/glmv1/pi.html#initial-insights",
    "href": "sections/glmv1/pi.html#initial-insights",
    "title": "Project Overview",
    "section": "Initial Insights:",
    "text": "Initial Insights:\nThe inventory data is comprised of 488 distinct items, sold over a span of 54 weeks (from week 0 to 53), with an impressive sum of approximately 1.3 million units transacted.\nTo visualize and better understand the distribution of sales, I prepared a series of graphical representations to discern patterns, identify anomalies, and appreciate the overall behavior of our sales data over time, thereby setting the stage for the predictive analysis.\n\n\nCode\n# import data\nsai &lt;-  read.csv(\"inventory.csv\")\n# import my custom theme :-)\nsource(\"artNov_theme.R\")\n\nhist &lt;- sai %&gt;% \n  group_by(item_no) %&gt;%\n  summarise(total_sold = sum(sold)) %&gt;% \n \nggplot(aes(x = total_sold)) +\n  geom_histogram(binwidth = 2100, color = \"black\", fill = \"#7D998F\", \n                 alpha = .6) +\n        scale_x_continuous(labels = scales::comma_format()) +\n  \n  labs(x = \"Number of Items Sold\", y = \"Frequency\", \n       title = \"Number of Items Sold by Item Type\", \n       caption =  \" figure 1.1\")+\n    theme_art_nouveau()\n\n\n\nhist1 &lt;- sai %&gt;% \n  group_by(week) %&gt;%\n  summarise(total_sold = sum(sold)) %&gt;% \n \nggplot(aes(x = total_sold)) +\n  geom_histogram(binwidth =  2100, color = \"black\", fill = \"#68576D\", \n                 alpha = .6) +\n        scale_x_continuous(labels = scales::comma_format()) +\n        scale_y_continuous(position = \"right\") +\n  \n  labs(x = \"Number of Items Sold\", \n       title = \"Number of Items Sold by Week\",\n        caption =  \" figure 1.2\")+\n  theme_art_nouveau()\n\n# display the histograms side by side\ngridExtra::grid.arrange(hist, hist1, ncol = 2)\n\n\n\n\n\n\nfigure 1. The histograms demonstrate the distribution of the dataset by variable. The green histogram demonstrates the frequency of each item sold, with most items ranging between 500 and 1300 in total. The pink histogram conveys the temporal dispersion of weekly sales. Both graphs exhibit a right skew, particularly pronounced in the green histogram, implying a scarcity of high-sales items or weeks.\n\n\n\n\n\n\nCode\nline1 &lt;- sai %&gt;%\n  group_by(week) %&gt;%\n  summarise(total_sold = sum(sold)) %&gt;%\n  ggplot(aes(x = week, y = total_sold)) +\n  geom_line(size = 2, alpha = 0.5, color = \"#DF5875\") +\n  geom_point(size = 4, alpha = 0.5, color = \"#DF5875\")+\n  geom_area(fill = \"#DF5875\", alpha = 0.4) +\n  scale_x_continuous(breaks = seq(0, 55, by = 5)) + \n  scale_y_continuous(labels = scales::comma_format()) +\n   \n  labs(x = \"week\", y = \"Count of Items Sold\", \n       title = \"Number of Items Sold by Week\",\n        caption =  \" figure 2\")+\n  theme_art_nouveau()\nline1\n\n\n\n\n\n\n\nfigure 2. The line graph shows annual sales trends in the unprocessed data set. Weekly sales fluctuate within a range of 10,000 to 50,000 items, with the majority hovering between ~20k and ~30k units sold. A distinct sales peak is observed in week 20, while week 46 registers the slowest sales activity. This trend analysis aids in identifying key sales periods and provides valuable context for refining our predictive models"
  },
  {
    "objectID": "sections/glmv1/pi.html#model-evaluation",
    "href": "sections/glmv1/pi.html#model-evaluation",
    "title": "inventory prediction",
    "section": "Model Evaluation",
    "text": "Model Evaluation\nfind the best model using stepwise selection find the 2 best methods for every size predictors\n\n\nCode\n# https://youtu.be/IScjygOnO0w\nset.seed(3746)\n# forward Stepwise Selection\n# nvmax = 8 specifies the maximum number of predictors to incorporate in the model.\na5i_mod &lt;- regsubsets(sold ~  .,\n                       data = a5i, nbest = 2, method = \"exhaustive\")\na5i_mod_summary_mx &lt;- with(summary(a5i_mod), data.frame(rsq,adjr2, cp, rss, outmat))\n\na5i_mod_summary_mx$predictors_cnt &lt;- c(1,1,2,2,3,3,4,4,5) # for graphing\na5i_mod_summary_mx$model &lt;- c('a','b','a','b','a','b','a','b','a') # for graphing\n\n# insert the num_predictors column to a5i_mod_summary_mx\na5i_mod_summary_mx \n\n\n                rsq     adjr2        cp       rss month week lag2 lag1\n1  ( 1 ) 0.14502533 0.1275769 31.959462 146883607                     \n1  ( 2 ) 0.09746449 0.0790454 36.351847 155054501               *     \n2  ( 1 ) 0.38735758 0.3618308 11.579355 105251221               *     \n2  ( 2 ) 0.18497440 0.1510150 30.270046 140020406                    *\n3  ( 1 ) 0.49724866 0.4651582  3.430586  86372067               *    *\n3  ( 2 ) 0.38901570 0.3500167 13.426223 104966358     *         *     \n4  ( 1 ) 0.49802651 0.4543766  5.358750  86238434     *         *    *\n4  ( 2 ) 0.49758788 0.4538999  5.399259  86313791          *    *    *\n5  ( 1 ) 0.51273908 0.4585990  6.000000  83710833     *    *    *    *\n         moving_avg_4weeks predictors_cnt model\n1  ( 1 )                 *              1     a\n1  ( 2 )                                1     b\n2  ( 1 )                 *              2     a\n2  ( 2 )                 *              2     b\n3  ( 1 )                 *              3     a\n3  ( 2 )                 *              3     b\n4  ( 1 )                 *              4     a\n4  ( 2 )                 *              4     b\n5  ( 1 )                 *              5     a\n\n\n\n\nCode\na5i_mod_summary_mx %&gt;% \n  pivot_longer(c(adjr2, cp), names_to = \"metric\", values_to = \"value\") %&gt;%\n  ggplot(aes(x = predictors_cnt, y = value, color = model)) +\n  geom_line(show.legend = F) +\n  geom_point() +\n  facet_wrap(~metric, scales = \"free\") +\n  scale_x_continuous(breaks = 1:5)"
  },
  {
    "objectID": "sections/glmv1/pi.html#feature-engineering",
    "href": "sections/glmv1/pi.html#feature-engineering",
    "title": "Project Overview",
    "section": "Feature engineering",
    "text": "Feature engineering\nI crafted new variables to derive a better understanding of sales trends, with the hope of enhancing the predictive model’s performance. These variables provide valuable insights into sales trends, seasonality, and best-selling items, which can be used to enhance the predictive model’s performance and gain a better understanding of the data’s dynamics. Ideally, meeting with domain experts would be part of this process, to ensure the variables represent reality.\n\ndata dictionary\n\nitem_no: the unique identifier for each item\nweek: the week number. “sold”: number of items sold during a specific week\nyear_total:the total number of items sold for each item over the entire year\ndate: The specific date corresponding to a week\nmonth: the month corresponding to a specific week\nlag1: period for capturing seasonality or time-dependent patterns, item’s sold from the previous week\nlag2: item’s sold from two-weeks prior\nrolling_4wk_avg: the total number of items sold over the past four weeksby item_no\nbest_seller: A binary variable indicating whether the item is one of the top ten best-selling items based on volume\n\n\n\nCode\n#####\n# create the variables\n# total yearly sales for each item\ninventory_sum &lt;- sai %&gt;%\n  group_by(item_no) %&gt;%\n  summarise(year_total = sum(sold), .groups = \"drop\")\n\nsai &lt;- left_join(sai, inventory_sum, by = c(\"item_no\"))\n\n###\n# calculate the top 10% threshold\ntop_10_threshold &lt;- quantile(sai$year_total, 0.9)\n\n# dummy variable for bestsellers 1= yes 0 = no\nsai &lt;- sai %&gt;%\n  mutate(best_seller = ifelse(year_total &gt;= top_10_threshold, 1, 0))\n\n## figure out month an year for data, assuming the data is from  2022\n# week 0: December 27, 2021 - January 2, 2022\n# week 52: December 19, 2022 - December 25, 2022\n# week 53: December 26, 2022 - January 1, 2023\n\nsai$year &lt;- 2022\n\nsai &lt;- sai %&gt;%\n  mutate(calendar_week = ifelse(week == 0, 52, ifelse(week == 53, 1, week)),\n         calendar_year = ifelse(week == 0, year - 1, ifelse(week == 53, year + 1, year)))\n\n# Now use calendar_year and calendar_week in the MMWRweek2Date function\nsai$date &lt;- MMWRweek2Date(sai$calendar_year, sai$calendar_week)\n\n\n\n# find the year the week ends in, so we can figure out months\nsai &lt;- sai %&gt;%\n  mutate(wk_ending_year = ifelse(week %in% c(0, 53), year + 1, year),\n         week = ifelse(week == 53, 1, week))\n\n\n\n# create teh data and extract the month\n# wanted month to be quantitative to reduce model complexity\n\nsai$date &lt;- MMWRweek2Date(sai$wk_ending_year, sai$calendar_week)\nsai$month &lt;- month(sai$date, label = TRUE)\nsai$month &lt;- month(sai$date)\n\n\n# create lag periods\n# https://www.youtube.com/watch?v=Kn3llTjYS5E\n\nsai &lt;- sai %&gt;%\n  mutate(lag1 = lag(sold, 1),\n         lag2 = lag(sold, 2))\n\n# https://www.rdocumentation.org/packages/zoo/versions/1.8-12/topics/rollmean\nsai &lt;- sai %&gt;%\n  group_by(item_no) %&gt;%\n  mutate(rolling_4wk_avg = zoo::rollmean(sold, k = 4, fill = NA, align = \"right\")) %&gt;%\n  ungroup()\n\nwrite.csv(sai, \"FEinventory.csv\")\n\n\n\n\nmonth_abbreviations &lt;- c(\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\")\n\nsai_average &lt;-  sai %&gt;%\n  filter(wk_ending_year == 2022) %&gt;% \n  group_by(month) %&gt;% \n  summarise(month_average = mean(sold))\n\n\n\nbar &lt;- sai %&gt;%\n    filter(wk_ending_year == 2022) %&gt;% \n    mutate(month = factor(month, levels = 1:12, labels = month_abbreviations)) %&gt;%\n  ggplot(aes(x = month, y = sold)) +\n  geom_bar(stat = \"sum\", fill = \"#68576D\", alpha = 0.75) +\n  labs(x = \"Month\", y = \"Total Sold\", title = \"Total Sold by Month\",\n       caption =  \" figure 4.1\") +\n  scale_y_continuous(labels = scales::comma_format()) +\n  theme_art_nouveau() +\n  theme(legend.position = \"none\")\n\ndot &lt;-  sai_average %&gt;%\n  mutate(month = factor(month, levels = 1:12, labels = month_abbreviations)) %&gt;%\n  ggplot(aes(x = month, y = month_average)) +\n  geom_point(size = 3) +\n  geom_point(size = 4, shape = 8, alpha = 0.75) +\n  geom_text(aes(label = round(month_average, 2)), nudge_y = 1.5) +  # Adjust nudge_y value for closer annotations\n  scale_y_continuous(position = \"right\") +\n  labs(x = \"Month\", y = \"\", title = \"Average Items Sold by Month\", caption = \"Figure 4.2\") +\n  theme_art_nouveau() +\n  theme(axis.text.y = element_blank())  # Remove y-axis ticks\n \n  \n\ngridExtra::grid.arrange(bar, dot, ncol = 2)\n\n\n\n\n\n\nfigure 4. The bar graph displays the total sales volume by month. July stands out as the peak season, whereas November and December show lower inventory movement. U. However, the average number of items sold in a month is relatively low compared to the totals. This suggests the presence of numerous zeros, which are affecting the overall average.\n\n\nCode\n# #https://www.geeksforgeeks.org/visualization-of-a-correlation-matrix-using-ggplot2-in-r/\n# theme_art_nov_minimal &lt;- function() {\n#   theme(\n#     text = element_text(family = 'EB Garamond'),\n#     panel.grid = element_blank(),\n#     panel.border = element_blank(),\n#     panel.background = element_rect(fill = \"#FEE8E1\", color = \"#DF5875\"),\n#     panel.grid.major = element_line(color = \"#DF5875\", linetype = \"dotted\", linewidth = 0.5),\n#     panel.grid.minor = element_blank(),\n#     axis.line = element_line(color = \"#DF5875\", linewidth = 1),\n#     legend.position = \"bottom\"\n#   )\n# }\n# \n# \n# sai %&gt;%\n#   select(week, month, lag1, lag2, rolling_4wk_avg, best_seller, sold) %&gt;%\n#   ggpairs(\n#     upper = list(continuous = \"cor\"),\n#     lower = list(continuous = \"points\"),\n#     diag = list(continuous = \"densityDiag\")\n#   ) +\n#   theme_art_nov_minimal()\n\n\n\n\nfigure 5. The corelation matrix.\n\n\n\n ### Splitting the Data With the variables now set, it’s time to divide the data for the training and testing of the predictive model. Ideally, the previous year would serve as a training ground, providing a foundation for testing the model. However, due to the critical nature of maintaining the temporal order, I segmented the data by inventory type, 75% into training to test on the remaining 25%.\n\n\nCode\n# create a list of all items to randomly select\nset.seed(1985)\nitems_sample_list &lt;-  unique(sai$item_no) %&gt;% \n  sample(366)\n# 75% of the data\n\n\n# splitting the inventory by item type to keep the temporal order\n\nis_train &lt;-  sai %&gt;% \n  filter(item_no %in% items_sample_list) %&gt;% \n  dplyr::select(\n    item_no,\n    week,\n    month,\n    lag1,\n    lag2,\n    rolling_4wk_avg,\n    best_seller, \n    sold) #not sure why i had to include the dplyr package\n\n\nis_test&lt;-  sai %&gt;% \n  filter(!(item_no %in% items_sample_list)) %&gt;% \n  dplyr::select(\n    item_no,\n    week,\n    month,\n    lag1,\n    lag2,\n    rolling_4wk_avg,\n    best_seller, \n    sold) #not sure why i had to include the dplyr package"
  },
  {
    "objectID": "sections/glmv1/pi.html#model-selection",
    "href": "sections/glmv1/pi.html#model-selection",
    "title": "inventory prediction",
    "section": "Model Selection",
    "text": "Model Selection\nfind the best model using stepwise selection find the 2 best methods for every size predictors reasons to use stepwise with caution - reason a - reason b - reason c\n\ndefine generalized linear regression and why I chose this for the model\n\n\n\nCode\n# https://youtu.be/IScjygOnO0w\n# forward Stepwise Selection\n# nvmax = 8 specifies the maximum number of predictors to incorporate in the model.\nis_mod &lt;- regsubsets(sold ~ week + month + lag1 + lag2 + rolling_4wk_avg + best_seller,\n                       data = is_test, nbest = 2, method = \"exhaustive\")\nis_mod_summary_mx &lt;- with(summary(is_mod), data.frame(rsq,adjr2, cp, rss, outmat))\n\nis_mod_summary_mx$predictors &lt;- c(1,1,2,2,3,3,4,4,5,5,6) # for graphing\nis_mod_summary_mx$model &lt;- c('a','b','a','b','a','b','a','b','a', 'b', 'a') # for graphing\n\n\n\n\nCode\nis_mod_summary_mx %&gt;% \n gt(rowname_col = \"predictors\") %&gt;%\n  tab_header(title = md(\"model comparison\")) %&gt;% \n  cols_align(align = \"left\") %&gt;% \n  data_color(\n    columns = cp,\n    palette = c(\"#15050B\", \"#F6D5E2\",\"#D35C9E\" ),\n    domain = c(0, 4000),\n    alpha = 0.8\n  ) %&gt;% \n    tab_options(\n    grand_summary_row.background.color = \"#D35C9E\",\n    heading.background.color = \"#EFFBFC\",\n    column_labels.background.color = \"#EFFBFC\",\n    stub.background.color = \"#EFFBFC\",\n    table.font.color = \"#323232\",\n    stub.border.style = \"dashed\",\n    stub.border.width = \"1px\",\n    table.width = \"60%\"\n  ) %&gt;%\n  opt_all_caps()\n\n\nWarning: Some values were outside the color scale and will be treated as NA\n\n\n\n\n\n\n  \n    \n      model comparison\n    \n    \n    \n      \n      rsq\n      adjr2\n      cp\n      rss\n      week\n      month\n      lag1\n      lag2\n      rolling_4wk_avg\n      best_seller\n      model\n    \n  \n  \n    1\n0.35517493\n0.35507126\n3641.238692\n347410480\n \n \n \n \n*\n \na\n    1\n0.07253195\n0.07238283\n7962.790093\n499689200\n \n \n \n \n \n*\nb\n    2\n0.47178311\n0.47161324\n1860.324363\n284585840\n \n \n \n*\n*\n \na\n    2\n0.40971495\n0.40952512\n2809.333361\n318026117\n \n \n*\n \n*\n \nb\n    3\n0.59311658\n0.59292027\n7.161336\n219215369\n \n \n*\n*\n*\n \na\n    3\n0.47179023\n0.47153538\n1862.215571\n284582007\n \n*\n \n*\n*\n \nb\n    4\n0.59312292\n0.59286113\n9.064396\n219211953\n \n*\n*\n*\n*\n \na\n    4\n0.59311764\n0.59285586\n9.145038\n219214795\n \n \n*\n*\n*\n*\nb\n    5\n0.59351846\n0.59319149\n5.016718\n218998851\n*\n*\n*\n*\n*\n \na\n    5\n0.59312400\n0.59279672\n11.047895\n219211372\n \n*\n*\n*\n*\n*\nb\n    6\n0.59351955\n0.59312713\n7.000000\n218998262\n*\n*\n*\n*\n*\n*\na\n  \n  \n  \n\n\n\n\nCode\n# update this too  look more like my other graphs\n#https://posit.co/blog/great-looking-tables-gt-0-2/# turn results into a pretty table\n# is_mod_summary_mx\n\n\n\nmodel evaluation\n\n\nCode\n# make tghis chart prettier\ngcolors &lt;- c(\"#68576D\", \"#DF5875\")\n\nis_mod_summary_mx %&gt;%\n  pivot_longer(c(adjr2, cp), names_to = \"metric\", values_to = \"value\") %&gt;%\n  ggplot(aes(x = predictors, y = value, color = model)) +\n  geom_line(show.legend = FALSE) +\n  geom_point(size = 3, alpha = .8) +\n  facet_wrap(~metric, scales = \"free\") +\n  scale_x_continuous(breaks = 1:6) +\n  scale_color_manual(values = gcolors) +\n  labs(x = \"predictors\", y = \"metric\", title = \"Number of Predictors by ADJR2 and CP\",\n       caption =  \" figure e\") +\n  theme_art_nouveau() \n\n\n\n\n\nCode\n# how can i make the value label larger on this?\n\n\nI see that the more predictors the great the adjr2 and the lower the cp, which is be expected. there appears to be diminishing returns at 4 predictors.\nmodel a has more optimal score on this compared to model b\nwhy too many predictors are bad reason xyz\n[interpret the graph]\nexplain why this is the best one what do each of these things tell me? - rsq -&gt; highest r squared, explains the variability in the model - adjr2 - &gt; highest - cp -&gt; smallest - rss -&gt; smallest\nI want to take my models through one more level os scrunity. having trouble choosing, i spent a lot of time making my variables so lets look at AIC several models what is AIC? Lower AIC values indicate a better-fit model, and a model with a delta-AIC (the difference between the two AIC values being compared) of more than -2 is considered significantly better than the model it is being compared to\nfind the coefficients and (Akaike Information Criterion) AIC Multiple linear regression compare the three best models\n\n\nCode\n#https://emilhvitfeldt.github.io/ISLR-tidymodels-labs/03-linear-regression.html\n\n# tidy model way\n\n specs &lt;- linear_reg() %&gt;%\n  set_mode(\"regression\") %&gt;%\n  set_engine(\"lm\")\n\nlm_3 &lt;- specs %&gt;%\n  fit(sold~ lag1 + lag2 + rolling_4wk_avg , data = is_train)\n\nlm_4 &lt;- specs %&gt;%\n  fit(sold~ month + lag1 + lag2 + rolling_4wk_avg , data = is_train)\n\nlm_5 &lt;- specs %&gt;%\n  fit(sold~ month + week + lag1 + lag2 + rolling_4wk_avg , data = is_train)\n\n\nmod_compare &lt;-  rbind(glance(lm_3),glance(lm_4),glance(lm_5))\n\n\n\n\nCode\nmod_compare %&gt;% \n gt(rowname_col = \"df\") %&gt;%\n  tab_header(title = md(\"model comparison\")) %&gt;% \n  cols_align(align = \"left\") %&gt;% \n  data_color(\n    columns = AIC,\n    palette = c(\"#15050B\", \"#F6D5E2\",\"#D35C9E\" ),\n    domain = c( 235603.3, 235605.8),\n    alpha = 0.8\n  ) %&gt;% \n    tab_options(\n    grand_summary_row.background.color = \"#D35C9E\",\n    heading.background.color = \"#EFFBFC\",\n    column_labels.background.color = \"#EFFBFC\",\n    stub.background.color = \"#EFFBFC\",\n    table.font.color = \"#323232\",\n    stub.border.style = \"dashed\",\n    stub.border.width = \"1px\",\n    table.width = \"60%\"\n  ) %&gt;%\n  opt_all_caps()\n\n\n\n\n\n\n  \n    \n      model comparison\n    \n    \n    \n      \n      r.squared\n      adj.r.squared\n      sigma\n      statistic\n      p.value\n      logLik\n      AIC\n      BIC\n      deviance\n      df.residual\n      nobs\n    \n  \n  \n    3\n0.5969407\n0.5968760\n133.2103\n9212.961\n0\n-117796.7\n235603.3\n235642.5\n331157049\n18662\n18666\n    4\n0.5969504\n0.5968640\n133.2123\n6909.627\n0\n-117796.4\n235604.9\n235651.9\n331149135\n18661\n18666\n    5\n0.5969742\n0.5968663\n133.2119\n5527.954\n0\n-117795.9\n235605.8\n235660.6\n331129521\n18660\n18666\n  \n  \n  \n\n\n\n\ninterpret the results\n\n\nCode\ntrain_precict &lt;- bind_cols(\n  predict(lm_3, new_data = is_train, type = \"conf_int\"),\n  is_train\n) %&gt;%\n  select(item_no, week, sold, .pred_lower, .pred_upper)\n\n# graph the results\n\n\ncompare the train model against the test\n\n\nCode\nlm_3 &lt;- specs %&gt;%\n  fit(sold~ lag1 + lag2 + rolling_4wk_avg , data = is_train)\n\nlm_3t &lt;- specs %&gt;%\n  fit(sold~ lag1 + lag2 + rolling_4wk_avg , data = is_test)\n\ntt_compare &lt;-  rbind(glance(lm_3),glance(lm_3t))\ntt_compare$mod &lt;-  c(\"train\", \"test\")\n\n# create a visualization\n\n\n\n\nCode\nactual_predicted &lt;- bind_cols(\n  predict(lm_3t, new_data = is_train, type = \"conf_int\"),\n  is_train\n) %&gt;%\n  select(item_no, week, sold, .pred_lower, .pred_upper)\n\n\n\n\ngraph results"
  },
  {
    "objectID": "sections/glmv1/pi.html#description-of-data",
    "href": "sections/glmv1/pi.html#description-of-data",
    "title": "inventory prediction",
    "section": "Description of Data:",
    "text": "Description of Data:\nThe initial inventory data set contains 488 items sold over the course of 54 weeks (0-53)\nThe figures below distrbution of items sold by item type and week. The line graph shows the seasonality of the product\n\n\nCode\nhist &lt;- sai %&gt;% \n  group_by(item_no) %&gt;%\n  summarise(total_sold = sum(sold)) %&gt;% \n \nggplot(aes(x = total_sold)) +\n  geom_histogram(binwidth = 2100, color = \"black\", fill = \"#7D998F\", \n                 alpha = .6) +\n        scale_x_continuous(labels = scales::comma_format()) +\n  \n  labs(x = \"Number of Items Sold\", y = \"Count of Item Type\", \n       title = \"Number of Items Sold by Item Type\", \n       caption =  \" figure a\")+\n    theme_art_nouveau()\n\n\n\nhist1 &lt;- sai %&gt;% \n  group_by(week) %&gt;%\n  summarise(total_sold = sum(sold)) %&gt;% \n \nggplot(aes(x = total_sold)) +\n  geom_histogram(binwidth =  2100, color = \"black\", fill = \"#DF5875\", \n                 alpha = .6) +\n        scale_x_continuous(labels = scales::comma_format()) +\n  \n  labs(x = \"Number of Items Sold\", y = \"Count of Weeks\", \n       title = \"Number of Items Sold by Week\",\n        caption =  \" figure b\")+\n  theme_art_nouveau()\n\n# display the histograms side by side\ngridExtra::grid.arrange(hist, hist1, ncol = 2)\n\n\n\n\n\n\nWhat does this histogram tell me?\n\nsales per week is slightly more evenly distributed throughout the sales cycle\nthe average week sells ~25K\n\n\n\n\nCode\nline1 &lt;- sai %&gt;%\n  group_by(week) %&gt;%\n  summarise(total_sold = sum(sold)) %&gt;%\n  ggplot(aes(x = week, y = total_sold)) +\n  geom_line(size = 2, alpha = 0.5, color = \"#DF5875\") +\n  geom_point(size = 4, alpha = 0.5, color = \"#DF5875\")+\n  geom_area(fill = \"#DF5875\", alpha = 0.4) +\n  scale_x_continuous(breaks = seq(0, 55, by = 5)) +  # Set breaks from 0 to 55 in increments of 5\n  scale_y_continuous(labels = scales::comma_format()) +\n   \n  labs(x = \"week\", y = \"Count of Items Sold\", \n       title = \"Number of Items Sold by Week\",\n        caption =  \" figure c\")+\n  theme_art_nouveau()\nline1"
  },
  {
    "objectID": "sections/glmv1/pi.html#limitations",
    "href": "sections/glmv1/pi.html#limitations",
    "title": "inventory prediction",
    "section": "Limitations",
    "text": "Limitations\n12 months of data is not great for this preserves the temporal order. A more robost dataset to work on would include a couple years of data"
  },
  {
    "objectID": "sections/glmv1/pi2.html",
    "href": "sections/glmv1/pi2.html",
    "title": "inventory prediction",
    "section": "",
    "text": "Code\n# import data\nsai &lt;-  read.csv(\"inventory.csv\")\nDRAFT DRAFT OUTLINE"
  },
  {
    "objectID": "sections/glmv1/pi2.html#clear-description-of-data",
    "href": "sections/glmv1/pi2.html#clear-description-of-data",
    "title": "inventory prediction",
    "section": "Clear Description of Data:",
    "text": "Clear Description of Data:\nStart by providing a comprehensive summary of the dataset. This includes source information, timeframe, and the main purpose for collection. Each variable should be properly named and described. This could be presented in the form of a data dictionary.\n[insert]"
  },
  {
    "objectID": "sections/glmv1/pi2.html#statistical-summary",
    "href": "sections/glmv1/pi2.html#statistical-summary",
    "title": "inventory prediction",
    "section": "Statistical Summary:",
    "text": "Statistical Summary:\nProvide a statistical summary of the variables. This includes measures of central tendency (mean, median, mode), dispersion (range, standard deviation, variance), and distribution properties (skewness, kurtosis). This could be supplemented with summary tables and histograms.\n[insert]"
  },
  {
    "objectID": "sections/glmv1/pi2.html#visual-exploration",
    "href": "sections/glmv1/pi2.html#visual-exploration",
    "title": "inventory prediction",
    "section": "Visual Exploration:",
    "text": "Visual Exploration:\nUse visualizations to give an overview of the data. Histograms, box plots, scatter plots, or correlation heatmaps can be used to show relationships and trends within the data..\n[insert]"
  },
  {
    "objectID": "sections/glmv1/pi2.html#initial-insights",
    "href": "sections/glmv1/pi2.html#initial-insights",
    "title": "inventory prediction",
    "section": "Initial Insights:",
    "text": "Initial Insights:\nStart by providing a comprehensive summary of the dataset. This includes source information, timeframe, and the main purpose for collection. Each variable should be properly named and described. This could be presented in the form of a data dictionary.\n[insert]"
  },
  {
    "objectID": "sections/glmv1/pi2.html#feature-engineering",
    "href": "sections/glmv1/pi2.html#feature-engineering",
    "title": "inventory prediction",
    "section": "Feature engineering",
    "text": "Feature engineering\n(link to data dictionary)\n\ntotals for each item over the year\nwhich items are best selling? top ten items based on volume\nconvert weeks to dates so we can extract the month\nlag period for seasonality\nrolling mean\n\n\n\nCode\n# total yearly sales for each item\ninventory_sum &lt;- sai %&gt;%\n  group_by(item_no) %&gt;%\n  summarise(year_total = sum(sold), .groups = \"drop\")\n\nsai &lt;- left_join(sai, inventory_sum, by = c(\"item_no\"))\n\n###\n# calculate the top 10% threshold\ntop_10_threshold &lt;- quantile(sai$year_total, 0.9)\n\n# dummy variable for bestsellers 1= yes 0 = no\nsai &lt;- sai %&gt;% \n  mutate(best_seller = ifelse(year_total &gt;= top_10_threshold, 1, 0))\n\n### figure out month an year for data, assuming the data is from  2022\n#week 0: December 27, 2021 - January 2, 2022\n#week 52: December 19, 2022 - December 25, 2022\n#week 53: December 26, 2022 - January 1, 2023\n\nsai$year &lt;- 2022\n# find the year the week ends in, so we can figure out months\nsai &lt;- sai %&gt;%\n  mutate(wk_ending_year = ifelse(week %in% c(0, 53), year + 1, year),\n         week = ifelse(week == 0, 52, ifelse(week == 53, 1, week)))\n\n\n# create teh data and extract the month\n# wanted month to be quantitative to reduce model complexity \n\nsai$date &lt;- MMWRweek2Date(sai$wk_ending_year, sai$week)\nsai$month &lt;- month(sai$date, label = TRUE)\nsai$month &lt;- month(sai$date)\n\n\n# create lag periods\n# https://www.youtube.com/watch?v=Kn3llTjYS5E\n\nsai &lt;- sai %&gt;%\n  mutate(lag1 = lag(sold, 1),\n         lag2 = lag(sold, 2))\n\n# https://www.rdocumentation.org/packages/zoo/versions/1.8-12/topics/rollmean\nsai &lt;- sai %&gt;%\n  group_by(item_no) %&gt;% \n  mutate(rolling_4wk_avg = zoo::rollmean(sold, k = 4, fill = NA, align = \"right\")) %&gt;%\n  ungroup()\n\n\n\n\nCode\n# validate data\nwrite.csv(sai, \"FEinventory.csv\")\n\n\n\n\nCode\na5i &lt;-  sai %&gt;% \n  filter(item_no == 'A510004') %&gt;% \n  dplyr::select(\n    week,\n    month,\n    lag1,\n    lag2,\n    rolling_4wk_avg,\n    sold) #not sure why i had to include the dplyr package\n\n\ncreate a model based on the best selling item for reasons XYZ"
  },
  {
    "objectID": "sections/glmv1/pi2.html#model-selection",
    "href": "sections/glmv1/pi2.html#model-selection",
    "title": "inventory prediction",
    "section": "Model Selection",
    "text": "Model Selection\nfind the best model using stepwise selection find the 2 best methods for every size predictors\n\n\nCode\n# https://youtu.be/IScjygOnO0w\nset.seed(3746)\n# forward Stepwise Selection\n# nvmax = 8 specifies the maximum number of predictors to incorporate in the model.\na5i_mod &lt;- regsubsets(sold ~  .,\n                       data = a5i, nbest = 2, method = \"exhaustive\")\na5i_mod_summary_mx &lt;- with(summary(a5i_mod), data.frame(rsq,adjr2, cp, rss, outmat))\n\na5i_mod_summary_mx$predictors_cnt &lt;- c(1,1,2,2,3,3,4,4,5) # for graphing\na5i_mod_summary_mx$model &lt;- c('a','b','a','b','a','b','a','b','a') # for graphing\na5i_mod_summary_mx$modeltype &lt;-  \"lrm\"\n\n# insert the num_predictors column to a5i_mod_summary_mx\na5i_mod_summary_mx \n\n\n                rsq     adjr2        cp       rss week month lag1 lag2\n1  ( 1 ) 0.14502533 0.1275769 31.959462 146883607                     \n1  ( 2 ) 0.09746449 0.0790454 36.351847 155054501                    *\n2  ( 1 ) 0.38735758 0.3618308 11.579355 105251221                    *\n2  ( 2 ) 0.18497440 0.1510150 30.270046 140020406               *     \n3  ( 1 ) 0.49724866 0.4651582  3.430586  86372067               *    *\n3  ( 2 ) 0.38901570 0.3500167 13.426223 104966358          *         *\n4  ( 1 ) 0.49802651 0.4543766  5.358750  86238434          *    *    *\n4  ( 2 ) 0.49758788 0.4538999  5.399259  86313791    *          *    *\n5  ( 1 ) 0.51273908 0.4585990  6.000000  83710833    *     *    *    *\n         rolling_4wk_avg predictors_cnt model modeltype\n1  ( 1 )               *              1     a       lrm\n1  ( 2 )                              1     b       lrm\n2  ( 1 )               *              2     a       lrm\n2  ( 2 )               *              2     b       lrm\n3  ( 1 )               *              3     a       lrm\n3  ( 2 )               *              3     b       lrm\n4  ( 1 )               *              4     a       lrm\n4  ( 2 )               *              4     b       lrm\n5  ( 1 )               *              5     a       lrm\n\n\n\nmodel evaluation\n\n\nCode\na5i_mod_summary_mx %&gt;% \n  pivot_longer(c(adjr2, cp), names_to = \"metric\", values_to = \"value\") %&gt;%\n  ggplot(aes(x = predictors_cnt, y = value, color = model)) +\n  geom_line(show.legend = F) +\n  geom_point() +\n  facet_wrap(~metric, scales = \"free\") +\n  scale_x_continuous(breaks = 1:5)"
  },
  {
    "objectID": "sections/glmv1/pi3.html",
    "href": "sections/glmv1/pi3.html",
    "title": "inventory prediction",
    "section": "",
    "text": "Code\n# import data\nsai &lt;-  read.csv(\"inventory.csv\")\nDRAFT DRAFT OUTLINE"
  },
  {
    "objectID": "sections/glmv1/pi3.html#description-of-data",
    "href": "sections/glmv1/pi3.html#description-of-data",
    "title": "inventory prediction",
    "section": "Description of Data:",
    "text": "Description of Data:\nThe initial inventory dataset contains 26,352 items sold over the course of 54 weeks (0-53)\n[insert histogram of items sold]"
  },
  {
    "objectID": "sections/glmv1/pi3.html#limitations",
    "href": "sections/glmv1/pi3.html#limitations",
    "title": "inventory prediction",
    "section": "Limitations",
    "text": "Limitations\n12 months of data is not great for this preserves the temporal order. A more robost dataset to work on would include a couple years of data"
  },
  {
    "objectID": "sections/glmv1/pi3.html#statistical-summary",
    "href": "sections/glmv1/pi3.html#statistical-summary",
    "title": "inventory prediction",
    "section": "Statistical Summary:",
    "text": "Statistical Summary:\nProvide a statistical summary of the variables. This includes measures of central tendency (mean, median, mode), dispersion (range, standard deviation, variance), and distribution properties (skewness, kurtosis). This could be supplemented with summary tables and histograms.\n[insert]"
  },
  {
    "objectID": "sections/glmv1/pi3.html#visual-exploration",
    "href": "sections/glmv1/pi3.html#visual-exploration",
    "title": "inventory prediction",
    "section": "Visual Exploration:",
    "text": "Visual Exploration:\nUse visualizations to give an overview of the data. Histograms, box plots, scatter plots, or correlation heatmaps can be used to show relationships and trends within the data..\n[insert]"
  },
  {
    "objectID": "sections/glmv1/pi3.html#initial-insights",
    "href": "sections/glmv1/pi3.html#initial-insights",
    "title": "inventory prediction",
    "section": "Initial Insights:",
    "text": "Initial Insights:\nStart by providing a comprehensive summary of the dataset. This includes source information, timeframe, and the main purpose for collection. Each variable should be properly named and described. This could be presented in the form of a data dictionary.\n[insert]"
  },
  {
    "objectID": "sections/glmv1/pi3.html#feature-engineering",
    "href": "sections/glmv1/pi3.html#feature-engineering",
    "title": "inventory prediction",
    "section": "Feature engineering",
    "text": "Feature engineering\nI needed to create a few more variables help with the predictions:\n(link to data dictionary)\n\ntotals for each item over the year\nwhich items are best selling? top ten items based on volume\nconvert weeks to dates so we can extract the month\nlag period for seasonality\nrolling mean\n\n\n\nCode\n# total yearly sales for each item\ninventory_sum &lt;- sai %&gt;%\n  group_by(item_no) %&gt;%\n  summarise(year_total = sum(sold), .groups = \"drop\")\n\nsai &lt;- left_join(sai, inventory_sum, by = c(\"item_no\"))\n\n###\n# calculate the top 10% threshold\ntop_10_threshold &lt;- quantile(sai$year_total, 0.9)\n\n# dummy variable for bestsellers 1= yes 0 = no\nsai &lt;- sai %&gt;%\n  mutate(best_seller = ifelse(year_total &gt;= top_10_threshold, 1, 0))\n\n## figure out month an year for data, assuming the data is from  2022\n# week 0: December 27, 2021 - January 2, 2022\n# week 52: December 19, 2022 - December 25, 2022\n# week 53: December 26, 2022 - January 1, 2023\n\nsai$year &lt;- 2022\n\nsai &lt;- sai %&gt;%\n  mutate(calendar_week = ifelse(week == 0, 52, ifelse(week == 53, 1, week)),\n         calendar_year = ifelse(week == 0, year - 1, ifelse(week == 53, year + 1, year)))\n\n# Now use calendar_year and calendar_week in the MMWRweek2Date function\nsai$date &lt;- MMWRweek2Date(sai$calendar_year, sai$calendar_week)\n\n\n\n# find the year the week ends in, so we can figure out months\nsai &lt;- sai %&gt;%\n  mutate(wk_ending_year = ifelse(week %in% c(0, 53), year + 1, year),\n         week = ifelse(week == 53, 1, week))\n\n\n\n# create teh data and extract the month\n# wanted month to be quantitative to reduce model complexity\n\nsai$date &lt;- MMWRweek2Date(sai$wk_ending_year, sai$calendar_week)\nsai$month &lt;- month(sai$date, label = TRUE)\nsai$month &lt;- month(sai$date)\n\n\n# create lag periods\n# https://www.youtube.com/watch?v=Kn3llTjYS5E\n\nsai &lt;- sai %&gt;%\n  mutate(lag1 = lag(sold, 1),\n         lag2 = lag(sold, 2))\n\n# https://www.rdocumentation.org/packages/zoo/versions/1.8-12/topics/rollmean\nsai &lt;- sai %&gt;%\n  group_by(item_no) %&gt;%\n  mutate(rolling_4wk_avg = zoo::rollmean(sold, k = 4, fill = NA, align = \"right\")) %&gt;%\n  ungroup()\n\n\n\n\nCode\nhead(sai)\n\n\n# A tibble: 6 × 14\n  item_no  week  sold year_total best_seller  year calendar_week calendar_year\n  &lt;chr&gt;   &lt;dbl&gt; &lt;int&gt;      &lt;int&gt;       &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;\n1 020-307     0     0        880           0  2022            52          2021\n2 020-307     1    80        880           0  2022             1          2022\n3 020-307     2     0        880           0  2022             2          2022\n4 020-307     3     0        880           0  2022             3          2022\n5 020-307     4     0        880           0  2022             4          2022\n6 020-307     5     0        880           0  2022             5          2022\n# ℹ 6 more variables: date &lt;date&gt;, wk_ending_year &lt;dbl&gt;, month &lt;dbl&gt;,\n#   lag1 &lt;int&gt;, lag2 &lt;int&gt;, rolling_4wk_avg &lt;dbl&gt;\n\n\nCode\ncolnames(sai)\n\n\n [1] \"item_no\"         \"week\"            \"sold\"            \"year_total\"     \n [5] \"best_seller\"     \"year\"            \"calendar_week\"   \"calendar_year\"  \n [9] \"date\"            \"wk_ending_year\"  \"month\"           \"lag1\"           \n[13] \"lag2\"            \"rolling_4wk_avg\"\n\n\nCode\n# validate data\nwrite.csv(sai, \"FEinventory.csv\")\n\n\n\n\nCode\na5i &lt;-  sai %&gt;% \n  filter(item_no == 'A510004') %&gt;% \n  dplyr::select(\n    week,\n    month,\n    lag1,\n    lag2,\n    rolling_4wk_avg,\n    sold) #not sure why i had to include the dplyr package\n\n\ncreate a model based on the best selling item, inventory with more items sold are more predcitable."
  },
  {
    "objectID": "sections/glmv1/pi3.html#model-selection",
    "href": "sections/glmv1/pi3.html#model-selection",
    "title": "inventory prediction",
    "section": "Model Selection",
    "text": "Model Selection\nfind the best model using stepwise selection find the 2 best methods for every size predictors\n\ndefine generalized linear regression and why I chose this for the model\ni removed the best sellers because all of the items were best sellers\n\n\n\nCode\n# https://youtu.be/IScjygOnO0w\nset.seed(3746)\n# forward Stepwise Selection\n# nvmax = 8 specifies the maximum number of predictors to incorporate in the model.\na5i_mod &lt;- regsubsets(sold ~  .,\n                       data = a5i, nbest = 2, method = \"exhaustive\")\na5i_mod_summary_mx &lt;- with(summary(a5i_mod), data.frame(rsq,adjr2, cp, rss, outmat))\n\na5i_mod_summary_mx$predictors_cnt &lt;- c(1,1,2,2,3,3,4,4,5) # for graphing\na5i_mod_summary_mx$model &lt;- c('a','b','a','b','a','b','a','b','a') # for graphing\na5i_mod_summary_mx$modeltype &lt;-  \"lrm\"\n\n# insert the num_predictors column to a5i_mod_summary_mx\na5i_mod_summary_mx \n\n\n                rsq     adjr2        cp       rss week month lag1 lag2\n1  ( 1 ) 0.14502533 0.1275769 31.959462 146883607                     \n1  ( 2 ) 0.09746449 0.0790454 36.351847 155054501                    *\n2  ( 1 ) 0.38735758 0.3618308 11.579355 105251221                    *\n2  ( 2 ) 0.18497440 0.1510150 30.270046 140020406               *     \n3  ( 1 ) 0.49724866 0.4651582  3.430586  86372067               *    *\n3  ( 2 ) 0.38901570 0.3500167 13.426223 104966358          *         *\n4  ( 1 ) 0.49802651 0.4543766  5.358750  86238434          *    *    *\n4  ( 2 ) 0.49758788 0.4538999  5.399259  86313791    *          *    *\n5  ( 1 ) 0.51273908 0.4585990  6.000000  83710833    *     *    *    *\n         rolling_4wk_avg predictors_cnt model modeltype\n1  ( 1 )               *              1     a       lrm\n1  ( 2 )                              1     b       lrm\n2  ( 1 )               *              2     a       lrm\n2  ( 2 )               *              2     b       lrm\n3  ( 1 )               *              3     a       lrm\n3  ( 2 )               *              3     b       lrm\n4  ( 1 )               *              4     a       lrm\n4  ( 2 )               *              4     b       lrm\n5  ( 1 )               *              5     a       lrm\n\n\n\nmodel evaluation\n\n\nCode\na5i_mod_summary_mx %&gt;% \n  pivot_longer(c(adjr2, cp), names_to = \"metric\", values_to = \"value\") %&gt;%\n  ggplot(aes(x = predictors_cnt, y = value, color = model)) +\n  geom_line(show.legend = F) +\n  geom_point() +\n  facet_wrap(~metric, scales = \"free\") +\n  scale_x_continuous(breaks = 1:5)"
  },
  {
    "objectID": "sections/dataviz/viz_placeholder.html",
    "href": "sections/dataviz/viz_placeholder.html",
    "title": "visualizations",
    "section": "",
    "text": "definition and image source: javaTpoint"
  },
  {
    "objectID": "sections/glmv1/pi.html#project-overview",
    "href": "sections/glmv1/pi.html#project-overview",
    "title": "inventory prediction",
    "section": "Project Overview",
    "text": "Project Overview\ngoal: predict inventory for the following year\nsteps: - exploratory analysis and descriptive analytics - feature engineer new variables - step-wise model selection - test and train a predictive model - interpret and evaluate the results - visualize the process"
  },
  {
    "objectID": "sections/glmv1/pi.html#intial-insights",
    "href": "sections/glmv1/pi.html#intial-insights",
    "title": "inventory prediction",
    "section": "Intial Insights:",
    "text": "Intial Insights:\nThe initial inventory data set contains 488 items sold over the course of 54 weeks (0-53)\nThe figures below distrbution of items sold by item type and week. The line graph shows the seasonality of the product\n\n\nCode\nhist &lt;- sai %&gt;% \n  group_by(item_no) %&gt;%\n  summarise(total_sold = sum(sold)) %&gt;% \n \nggplot(aes(x = total_sold)) +\n  geom_histogram(binwidth = 2100, color = \"black\", fill = \"#7D998F\", \n                 alpha = .6) +\n        scale_x_continuous(labels = scales::comma_format()) +\n  \n  labs(x = \"Number of Items Sold\", y = \"Count of Item Type\", \n       title = \"Number of Items Sold by Item Type\", \n       caption =  \" figure a\")+\n    theme_art_nouveau()\n\n\n\nhist1 &lt;- sai %&gt;% \n  group_by(week) %&gt;%\n  summarise(total_sold = sum(sold)) %&gt;% \n \nggplot(aes(x = total_sold)) +\n  geom_histogram(binwidth =  2100, color = \"black\", fill = \"#DF5875\", \n                 alpha = .6) +\n        scale_x_continuous(labels = scales::comma_format()) +\n        scale_y_continuous(position = \"right\") +\n  \n  labs(x = \"Number of Items Sold\", y = \"Count of Weeks\", \n       title = \"Number of Items Sold by Week\",\n        caption =  \" figure b\")+\n  theme_art_nouveau()\n\n# display the histograms side by side\ngridExtra::grid.arrange(hist, hist1, ncol = 2)\n\n\n\n\n\n\nWhat does this histogram tell me?\n\nsales per week is slightly more evenly distributed throughout the sales cycle\nthe average week sells ~25K\n\n\n\n\nCode\nline1 &lt;- sai %&gt;%\n  group_by(week) %&gt;%\n  summarise(total_sold = sum(sold)) %&gt;%\n  ggplot(aes(x = week, y = total_sold)) +\n  geom_line(size = 2, alpha = 0.5, color = \"#DF5875\") +\n  geom_point(size = 4, alpha = 0.5, color = \"#DF5875\")+\n  geom_area(fill = \"#DF5875\", alpha = 0.4) +\n  scale_x_continuous(breaks = seq(0, 55, by = 5)) + \n  scale_y_continuous(labels = scales::comma_format()) +\n   \n  labs(x = \"week\", y = \"Count of Items Sold\", \n       title = \"Number of Items Sold by Week\",\n        caption =  \" figure c\")+\n  theme_art_nouveau()\nline1"
  },
  {
    "objectID": "sections/glmv1/pi.html#model-feature-selection",
    "href": "sections/glmv1/pi.html#model-feature-selection",
    "title": "Project Overview",
    "section": "Model Feature Selection",
    "text": "Model Feature Selection\nThe next step of the project involves selecting the best model using stepwise selection. It’s an iterative process of adding and removing predictors based on their statistical significance. However, it should be used with caution. The inclusion of irrelevant variables can lead to unnecessary complexity in the resulting model and can inflate the variance of prediction errors.\n\n\nCode\n# https://youtu.be/IScjygOnO0w\n# forward Stepwise Selection\n# nvmax = 8 specifies the maximum number of predictors to incorporate in the model.\nis_mod &lt;- regsubsets(sold ~ week + month + lag1 + lag2 + rolling_4wk_avg + best_seller,\n                       data = is_test, nbest = 2, method = \"exhaustive\")\nis_mod_summary_mx &lt;- with(summary(is_mod), data.frame(rsq,adjr2, cp, rss, outmat))\n\nis_mod_summary_mx$predictors &lt;- c(1,1,2,2,3,3,4,4,5,5,6) # for graphing\nis_mod_summary_mx$model &lt;- c('a','b','a','b','a','b','a','b','a', 'b', 'a') # for graphing\n\n\nfor model six show the table of coefficient, std. error, t-statistic, p-value\n\n\nCode\n# is_mod_summary_mx %&gt;% \n#  gt(rowname_col = \"predictors\") %&gt;%\n#   tab_header(title = md(\"model comparison\")) %&gt;% \n#   cols_align(align = \"left\") %&gt;% \n#   data_color(\n#     columns = cp,\n#     palette = c(\"#15050B\", \"#F6D5E2\",\"#D35C9E\" ),\n#     domain = c(0, 4000),\n#     alpha = 0.8\n#   ) %&gt;% \n#     tab_options(\n#     grand_summary_row.background.color = \"#D35C9E\",\n#     heading.background.color = \"#EFFBFC\",\n#     column_labels.background.color = \"#EFFBFC\",\n#     stub.background.color = \"#EFFBFC\",\n#     table.font.color = \"#323232\",\n#     stub.border.style = \"dashed\",\n#     stub.border.width = \"1px\",\n#     table.width = \"60%\"\n#   ) %&gt;%\n#   opt_all_caps()\n\n# update this too  look more like my other graphs\n#https://posit.co/blog/great-looking-tables-gt-0-2/# turn results into a pretty table\n# is_mod_summary_mx\n\nis_mod_summary_mx %&gt;% \n  mutate(rsq = round(rsq, 1), \n         adjr2 = round(adjr2, 1), \n         cp = scales::comma(round(cp, 1)),\n         rss = scales::comma(rss)) %&gt;%\n  rename(`4WK ROLLING AVG` = rolling_4wk_avg, `BEST SELLER` = best_seller) %&gt;% \n  select(predictors, model, everything()) %&gt;%\n  gt(rowname_col = \"predictors\") %&gt;%\n  tab_header(title = md(\"model comparison\")) %&gt;% \n    tab_options(\n    grand_summary_row.background.color = \"#D35C9E\",\n    heading.background.color = \"#EFFBFC\",\n    column_labels.background.color = \"#EFFBFC\",\n    stub.background.color = \"#EFFBFC\",\n    table.font.color = \"#323232\",\n    stub.border.style = \"dashed\",\n    stub.border.width = \"1px\",\n    table.width = \"60%\"\n  ) %&gt;%\n  opt_all_caps()\n\n\n\n\n\n\n  \n    \n      model comparison\n    \n    \n    \n      \n      model\n      rsq\n      adjr2\n      cp\n      rss\n      week\n      month\n      lag1\n      lag2\n      4WK ROLLING AVG\n      BEST SELLER\n    \n  \n  \n    1\na\n0.4\n0.4\n3,641.2\n347,410,480\n \n \n \n \n*\n \n    1\nb\n0.1\n0.1\n7,962.8\n499,689,200\n \n \n \n \n \n*\n    2\na\n0.5\n0.5\n1,860.3\n284,585,840\n \n \n \n*\n*\n \n    2\nb\n0.4\n0.4\n2,809.3\n318,026,117\n \n \n*\n \n*\n \n    3\na\n0.6\n0.6\n7.2\n219,215,369\n \n \n*\n*\n*\n \n    3\nb\n0.5\n0.5\n1,862.2\n284,582,007\n \n*\n \n*\n*\n \n    4\na\n0.6\n0.6\n9.1\n219,211,953\n \n*\n*\n*\n*\n \n    4\nb\n0.6\n0.6\n9.1\n219,214,795\n \n \n*\n*\n*\n*\n    5\na\n0.6\n0.6\n5.0\n218,998,851\n*\n*\n*\n*\n*\n \n    5\nb\n0.6\n0.6\n11.0\n219,211,372\n \n*\n*\n*\n*\n*\n    6\na\n0.6\n0.6\n7.0\n218,998,262\n*\n*\n*\n*\n*\n*\n  \n  \n  \n\n\n\n\n\ntable 1. the stepwise selection went through an exhaustive process to chose the best 2 models by number of predictors\n\n\nmodel evaluation\n\n\nCode\n# make tghis chart prettier\ngcolors &lt;- c(\"#68576D\", \"#DF5875\")\n\nis_mod_summary_mx %&gt;%\n  pivot_longer(c(adjr2, cp), names_to = \"metric\", values_to = \"value\") %&gt;%\n  ggplot(aes(x = predictors, y = value, color = model)) +\n  geom_line(show.legend = FALSE) +\n  geom_point(size = 3, alpha = .8) +\n  facet_wrap(~metric, scales = \"free\") +\n  scale_x_continuous(breaks = 1:6) +\n  scale_color_manual(values = gcolors) +\n  labs(x = \"predictors\", y = \"metric\", title = \"Number of Predictors by ADJR2 and CP\",\n       caption =  \" figure e\") +\n  theme_art_nouveau() \n\n\n\n\n\nCode\n# how can i make the value label larger on this?\n\n\nI see that the more predictors the great the adjr2 and the lower the cp, which is be expected. there appears to be diminishing returns at 4 predictors.\nmodel a has more optimal score on this compared to model b\nwhy too many predictors are bad reason xyz\n[interpret the graph]\nexplain why this is the best one what do each of these things tell me? - rsq -&gt; highest r squared, explains the variability in the model - adjr2 - &gt; highest - cp -&gt; smallest - rss -&gt; smallest\nI want to take my models through one more level os scrunity. having trouble choosing, i spent a lot of time making my variables so lets look at AIC several models what is AIC? Lower AIC values indicate a better-fit model, and a model with a delta-AIC (the difference between the two AIC values being compared) of more than -2 is considered significantly better than the model it is being compared to\nfind the coefficients and (Akaike Information Criterion) AIC Multiple linear regression compare the three best models\n\n\nCode\n#https://emilhvitfeldt.github.io/ISLR-tidymodels-labs/03-linear-regression.html\n\n# tidy model way\n\n specs &lt;- linear_reg() %&gt;%\n  set_mode(\"regression\") %&gt;%\n  set_engine(\"lm\")\n\nlm_3 &lt;- specs %&gt;%\n  fit(sold~ lag1 + lag2 + rolling_4wk_avg , data = is_train)\n\nlm_4 &lt;- specs %&gt;%\n  fit(sold~ month + lag1 + lag2 + rolling_4wk_avg , data = is_train)\n\nlm_5 &lt;- specs %&gt;%\n  fit(sold~ month + week + lag1 + lag2 + rolling_4wk_avg , data = is_train)\n\n\nmod_compare &lt;-  rbind(glance(lm_3),glance(lm_4),glance(lm_5))\n\n\n\n\nCode\nmod_compare %&gt;% \n gt(rowname_col = \"df\") %&gt;%\n  tab_header(title = md(\"model comparison\")) %&gt;% \n  cols_align(align = \"left\") %&gt;% \n  data_color(\n    columns = AIC,\n    palette = c(\"#15050B\", \"#F6D5E2\",\"#D35C9E\" ),\n    domain = c( 235603.3, 235605.8),\n    alpha = 0.8\n  ) %&gt;% \n    tab_options(\n    grand_summary_row.background.color = \"#D35C9E\",\n    heading.background.color = \"#EFFBFC\",\n    column_labels.background.color = \"#EFFBFC\",\n    stub.background.color = \"#EFFBFC\",\n    table.font.color = \"#323232\",\n    stub.border.style = \"dashed\",\n    stub.border.width = \"1px\",\n    table.width = \"60%\"\n  ) %&gt;%\n  opt_all_caps()\n\n\n\n\n\n\n  \n    \n      model comparison\n    \n    \n    \n      \n      r.squared\n      adj.r.squared\n      sigma\n      statistic\n      p.value\n      logLik\n      AIC\n      BIC\n      deviance\n      df.residual\n      nobs\n    \n  \n  \n    3\n0.5969407\n0.5968760\n133.2103\n9212.961\n0\n-117796.7\n235603.3\n235642.5\n331157049\n18662\n18666\n    4\n0.5969504\n0.5968640\n133.2123\n6909.627\n0\n-117796.4\n235604.9\n235651.9\n331149135\n18661\n18666\n    5\n0.5969742\n0.5968663\n133.2119\n5527.954\n0\n-117795.9\n235605.8\n235660.6\n331129521\n18660\n18666\n  \n  \n  \n\n\n\n\ninterpret the results\n\n\nCode\ntrain_precict &lt;- bind_cols(\n  predict(lm_3, new_data = is_train, type = \"conf_int\"),\n  is_train\n) %&gt;%\n  select(item_no, week, sold, .pred_lower, .pred_upper)\n\n# graph the results\n\n\ncompare the train model against the test\n\n\nCode\nlm_3 &lt;- specs %&gt;%\n  fit(sold~ lag1 + lag2 + rolling_4wk_avg , data = is_train)\n\nlm_3t &lt;- specs %&gt;%\n  fit(sold~ lag1 + lag2 + rolling_4wk_avg , data = is_test)\n\ntt_compare &lt;-  rbind(glance(lm_3),glance(lm_3t))\ntt_compare$mod &lt;-  c(\"train\", \"test\")\n\n# create a visualization\n\n\n\n\nCode\nactual_predicted &lt;- bind_cols(\n  predict(lm_3t, new_data = is_train, type = \"conf_int\"),\n  is_train\n) %&gt;%\n  select(item_no, week, sold, .pred_lower, .pred_upper)\n\n\n\n\ngraph results"
  },
  {
    "objectID": "sections/glmv1/pi.html#data-processing",
    "href": "sections/glmv1/pi.html#data-processing",
    "title": "Project Overview",
    "section": "Data Processing",
    "text": "Data Processing\nI feature engineered new variables to derive a better understanding of sales trends and enhance the predictive model’s performance. These variables provide valuable insights into sales trends, seasonality, and best-selling items, which can be used to enhance the predictive model’s performance and gain a better understanding of the data’s dynamics. Ideally, meeting with domain experts would be part of this process, to ensure the variables represent reality.\n\ndata dictionary\n\nitem_no: the unique identifier for each item\nweek: the week number. “sold”: number of items sold during a specific week\nyear_total:the total number of items sold for each item over the entire year\ndate: The specific date corresponding to a week\nmonth: the month corresponding to a specific week\nlag1: period for capturing seasonality or time-dependent patterns, item’s sold from the previous week\nlag2: item’s sold from two-weeks prior\nrolling_4wk_avg: the total number of items sold over the past four weeksby item_no\nbest_seller: A binary variable indicating whether the item is one of the top ten best-selling items based on volume\n\n\n\nCode\n#####\n# create the variables\n# total yearly sales for each item\ninventory_sum &lt;- sai %&gt;%\n  group_by(item_no) %&gt;%\n  summarise(year_total = sum(sold), .groups = \"drop\")\n\nsai &lt;- left_join(sai, inventory_sum, by = c(\"item_no\"))\n\n###\n# calculate the top 10% threshold\ntop_10_threshold &lt;- quantile(sai$year_total, 0.9)\n\n# dummy variable for bestsellers 1= yes 0 = no\nsai &lt;- sai %&gt;%\n  mutate(best_seller = ifelse(year_total &gt;= top_10_threshold, 1, 0))\n\n## figure out month an year for data, assuming the data is from  2022\n# week 0: December 27, 2021 - January 2, 2022\n# week 52: December 19, 2022 - December 25, 2022\n# week 53: December 26, 2022 - January 1, 2023\n\nsai$year &lt;- 2022\n\nsai &lt;- sai %&gt;%\n  mutate(calendar_week = ifelse(week == 0, 52, ifelse(week == 53, 1, week)),\n         calendar_year = ifelse(week == 0, year - 1, ifelse(week == 53, year + 1, year)))\n\n# Now use calendar_year and calendar_week in the MMWRweek2Date function\nsai$date &lt;- MMWRweek2Date(sai$calendar_year, sai$calendar_week)\n\n\n\n# find the year the week ends in, so we can figure out months\nsai &lt;- sai %&gt;%\n  mutate(wk_ending_year = ifelse(week %in% c(0, 53), year + 1, year),\n         week = ifelse(week == 53, 1, week))\n\n\n\n# create teh data and extract the month\n# wanted month to be quantitative to reduce model complexity\n\nsai$date &lt;- MMWRweek2Date(sai$wk_ending_year, sai$calendar_week)\nsai$month &lt;- month(sai$date, label = TRUE)\nsai$month &lt;- month(sai$date)\n\n\n# create lag periods\n# https://www.youtube.com/watch?v=Kn3llTjYS5E\n\nsai &lt;- sai %&gt;%\n  mutate(lag1 = lag(sold, 1),\n         lag2 = lag(sold, 2))\n\n# https://www.rdocumentation.org/packages/zoo/versions/1.8-12/topics/rollmean\nsai &lt;- sai %&gt;%\n  group_by(item_no) %&gt;%\n  mutate(rolling_4wk_avg = zoo::rollmean(sold, k = 4, fill = NA, align = \"right\")) %&gt;%\n  ungroup()\n\nwrite.csv(sai, \"FEinventory.csv\")\n\n\n\n\nmonth_abbreviations &lt;- c(\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\")\n\nsai_average &lt;-  sai %&gt;%\n  filter(wk_ending_year == 2022) %&gt;% \n  group_by(month) %&gt;% \n  summarise(month_average = mean(sold))\n\n\n\nbar &lt;- sai %&gt;%\n    filter(wk_ending_year == 2022) %&gt;% \n    mutate(month = factor(month, levels = 1:12, labels = month_abbreviations)) %&gt;%\n  ggplot(aes(x = month, y = sold)) +\n  geom_bar(stat = \"sum\", fill = \"#68576D\", alpha = 0.75) +\n  labs(x = \"Month\", y = \"Total Sold\", title = \"Total Sold by Month\",\n       caption =  \" figure 4.1\") +\n  scale_y_continuous(labels = scales::comma_format()) +\n  theme_art_nouveau() +\n  theme(legend.position = \"none\")\n\ndot &lt;-  sai_average %&gt;%\n  mutate(month = factor(month, levels = 1:12, labels = month_abbreviations)) %&gt;%\n  ggplot(aes(x = month, y = month_average)) +\n  geom_point(size = 3) +\n  geom_point(size = 4, shape = 8, alpha = 0.75) +\n  geom_text(aes(label = round(month_average, 2)), nudge_y = 1.5) +  #closer annotations\n  scale_y_continuous(position = \"right\") +\n  labs(x = \"Month\", y = \"\", title = \"Average Items Sold by Month\", caption = \"Figure 4.2\") +\n  theme_art_nouveau() +\n  theme(axis.text.y = element_blank())  \n \n  \n\ngridExtra::grid.arrange(bar, dot, ncol = 2)\n\n\n\n\n\n\nfigure 4. The bar graph displays the total sales volume by month. July stands out as the peak season, whereas November and December show lower inventory movement. U. However, the average number of items sold in a month is relatively low compared to the totals. This suggests the presence of numerous zeros, which are affecting the overall average.\n\n\nCode\n# #https://www.geeksforgeeks.org/visualization-of-a-correlation-matrix-using-ggplot2-in-r/\n# theme_art_nov_minimal &lt;- function() {\n#   theme(\n#     text = element_text(family = 'EB Garamond'),\n#     panel.grid = element_blank(),\n#     panel.border = element_blank(),\n#     panel.background = element_rect(fill = \"#FEE8E1\", color = \"#DF5875\"),\n#     panel.grid.major = element_line(color = \"#DF5875\", linetype = \"dotted\", linewidth = 0.5),\n#     panel.grid.minor = element_blank(),\n#     axis.line = element_line(color = \"#DF5875\", linewidth = 1),\n#     legend.position = \"bottom\"\n#   )\n# }\n# \n# \n# sai %&gt;%\n#   select(week, month, lag1, lag2, rolling_4wk_avg, best_seller, sold) %&gt;%\n#   ggpairs(\n#     upper = list(continuous = \"cor\"),\n#     lower = list(continuous = \"points\"),\n#     diag = list(continuous = \"densityDiag\")\n#   ) +\n#   theme_art_nov_minimal()\n\n\n\n\nfigure 5. The corelation matrix.\n\n\n\n\n\n\n\nSplitting the Data\nWith the variables now set, it’s time to divide the data for the training and testing of the predictive model. Ideally, the previous year would serve as a training ground, providing a foundation for testing the model. However, due to the critical nature of maintaining the temporal order, I segmented the data by inventory type, 75% into training to test on the remaining 25%.\n\n\nCode\n# create a list of all items to randomly select\nset.seed(1985)\nitems_sample_list &lt;-  unique(sai$item_no) %&gt;% \n  sample(366)\n# 75% of the data\n\n\n# splitting the inventory by item type to keep the temporal order\n\nis_train &lt;-  sai %&gt;% \n  filter(item_no %in% items_sample_list) %&gt;% \n  dplyr::select(\n    item_no,\n    week,\n    month,\n    lag1,\n    lag2,\n    rolling_4wk_avg,\n    best_seller, \n    sold) #not sure why i had to include the dplyr package\n\n\nis_test&lt;-  sai %&gt;% \n  filter(!(item_no %in% items_sample_list)) %&gt;% \n  dplyr::select(\n    item_no,\n    week,\n    month,\n    lag1,\n    lag2,\n    rolling_4wk_avg,\n    best_seller, \n    sold) #not sure why i had to include the dplyr package\n\n\n## story telling purposes only\n# https://r-graph-gallery.com/piechart-ggplot2.html\npiecha &lt;- data.frame(\n  split = c(\"train\", \"test\"),\n  percentage = c(0.75, 0.25)\n)\n\npiecha &lt;- piecha %&gt;%\n  arrange(desc(split)) %&gt;%\n  mutate(cumulative_percentage = cumsum(percentage) - percentage / 2,\n         label = paste(split, \": \", scales::percent(percentage)))\n\n# do you like fancy pie?\nggplot(piecha , aes(x=\"\", y=percentage, fill=split, label = label)) +\n  geom_bar(stat=\"identity\", width=2, color=\"white\", alpha=0.6) +\n  coord_polar(\"y\", start=0) +\n  scale_fill_manual(values=c(\"#68576D\", \"#DF5875\"), guide = \"none\") +\n  theme_art_nouveau() +\n  geom_text(aes(y = cumulative_percentage), color = \"white\", size = 7, face = \"bold\") +\n  theme(axis.title = element_blank(),\n        axis.text = element_blank()) +\n  ggtitle(\"Data Split for Modeling\") +\n  theme(plot.title = element_text(hjust = 0.5))"
  }
]